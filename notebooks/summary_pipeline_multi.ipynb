{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary pipeline: multi-strategy arbitrage event study\n",
    "\n",
    "This notebook builds a stacked panel of arbitrage spreads (TIPS–Treasury, Treasury spot–futures, CIP, equity spot–futures), merges common controls, and runs Layer 1 + Layer 2 designs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bb87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\notebooks\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "sys.path.insert(2, \"../src\")\n",
    "if 'src' in os.getcwd():\n",
    "    os.chdir(os.path.pardir)\n",
    "    print(os.getcwd())\n",
    "else:\n",
    "    print(os.getcwd())\n",
    "from slr_bucket.econometrics.event_study import add_event_time, event_study_regression, jump_estimator\n",
    "from slr_bucket.io import build_data_catalog, load_any_table, resolve_dataset_path, as_daily_date, coerce_num, keep_controls_with_coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b102352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\outputs\\summary_pipeline\\20260227_051043_cd72a0e15615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_3960\\3031162216.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  run_stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n"
     ]
    }
   ],
   "source": [
    "# CONFIG (multi-strategy)\n",
    "CONFIG = {\n",
    "    # Outcomes (loaded from data/series via slr_bucket.outcomes.stack_outcomes)\n",
    "    \"y_col\": \"y_abs_bps\",   # analyze dislocation magnitude; set to \"y_bps\" for signed\n",
    "    \"events\": [\"2020-04-01\", \"2021-03-19\", \"2021-03-31\"],\n",
    "    \"windows\": [20, 60],\n",
    "    \"event_bins\": [(-60,-41),(-40,-21),(-20,-1),(0,0),(1,20),(21,40),(41,60)],\n",
    "    # Controls\n",
    "    \"total_controls\": [\"VIX\", \"HY_OAS\", \"BAA10Y\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"],\n",
    "    \"direct_controls\": [\"VIX\", \"HY_OAS\", \"BAA10Y\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\", \"SOFR\", \"spr_tgcr\", \"spr_effr\"],\n",
    "    \"hac_lags\": 5,\n",
    "    \"run_layer2\": True,\n",
    "    # panel keys\n",
    "    \"fe_col\": \"series\",\n",
    "    \"strategy_col\": \"strategy\",\n",
    "    \"group_col\": \"treasury_based\",\n",
    "}\n",
    "cfg_hash = hashlib.sha256(json.dumps(CONFIG, sort_keys=True).encode()).hexdigest()[:12]\n",
    "run_stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "repo_root = Path.cwd().parent\n",
    "run_dir = repo_root / \"outputs\" / \"summary_pipeline\" / f\"{run_stamp}_{cfg_hash}\"\n",
    "(run_dir / \"tables\").mkdir(parents=True, exist_ok=True)\n",
    "(run_dir / \"figures\").mkdir(parents=True, exist_ok=True)\n",
    "(run_dir / \"data\").mkdir(parents=True, exist_ok=True)\n",
    "print(run_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de9257",
   "metadata": {},
   "source": [
    "## Data catalog and dataset inventory\n",
    "\n",
    "New `/data` structure uses layered folders (`raw`, `intermediate`, `series`). This run uses:\n",
    "- Outcomes: `data/series/tips_treasury_implied_rf_2010.(parquet|csv)` (`arb_*`).\n",
    "- Preferred merged controls: `data/intermediate/analysis_panel.csv` (if valid for required columns).\n",
    "- Fallback controls from raw inputs:\n",
    "  - `raw/event_inputs/controls_vix_creditspreads_fred`\n",
    "  - `raw/event_inputs/repo_rates_combined` (or `repo_rates_fred`)\n",
    "  - `raw/event_inputs/treasury_issuance_by_tenor_fiscaldata`\n",
    "- Layer 2 proxies (optional):\n",
    "  - `raw/event_inputs/primary_dealer_stats_ofr_stfm_nypd_long`\n",
    "  - `raw/event_inputs/bank_exposure_y9c_agg_daily.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b5def4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>layer</th>\n",
       "      <th>rows</th>\n",
       "      <th>columns</th>\n",
       "      <th>frequency</th>\n",
       "      <th>date_min</th>\n",
       "      <th>date_max</th>\n",
       "      <th>key_columns</th>\n",
       "      <th>join_hints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>5476</td>\n",
       "      <td>date,spread_2y_bps,spread_5y_bps,spread_10y_bp...</td>\n",
       "      <td>daily</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>date</td>\n",
       "      <td>daily:date | keys:date | layer:intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>420</td>\n",
       "      <td>date,bid_ask_spread,pubout,n_issues</td>\n",
       "      <td>monthly</td>\n",
       "      <td>1980-01-31</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>date</td>\n",
       "      <td>keys:date | layer:intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>1209</td>\n",
       "      <td>date,fed_assets</td>\n",
       "      <td>weekly</td>\n",
       "      <td>2002-12-18</td>\n",
       "      <td>2026-02-11</td>\n",
       "      <td>date</td>\n",
       "      <td>weekly:date | keys:date | layer:intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>1209</td>\n",
       "      <td>date,fed_treasury_holdings</td>\n",
       "      <td>weekly</td>\n",
       "      <td>2002-12-18</td>\n",
       "      <td>2026-02-11</td>\n",
       "      <td>date</td>\n",
       "      <td>weekly:date | keys:date | layer:intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>751</td>\n",
       "      <td>date,sofr,sofr_volume</td>\n",
       "      <td>daily</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>date</td>\n",
       "      <td>daily:date | keys:date | layer:intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>3752</td>\n",
       "      <td>date,spread_2y_bps,spread_5y_bps,spread_10y_bp...</td>\n",
       "      <td>daily</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>date</td>\n",
       "      <td>daily:date | keys:date | layer:intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\ra...</td>\n",
       "      <td>raw</td>\n",
       "      <td>3955</td>\n",
       "      <td>Date,AUD,CAD,CHF,EUR,GBP,JPY,NZD,SEK,USD</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td>layer:raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\ra...</td>\n",
       "      <td>raw</td>\n",
       "      <td>3913</td>\n",
       "      <td>('SPX Index', 'PX_LAST'),('SPX Index', 'IDX_ES...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td>layer:raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\ra...</td>\n",
       "      <td>raw</td>\n",
       "      <td>14</td>\n",
       "      <td>report_date,total_assets,total_reserves,total_...</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>report_date</td>\n",
       "      <td>quarterly:report_date | keys:report_date | lay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\ra...</td>\n",
       "      <td>raw</td>\n",
       "      <td>14</td>\n",
       "      <td>report_date,total_assets,total_reserves,total_...</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>report_date</td>\n",
       "      <td>quarterly:report_date | keys:report_date | lay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path         layer  rows  \\\n",
       "0  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...  intermediate  5476   \n",
       "1  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...  intermediate   420   \n",
       "2  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...  intermediate  1209   \n",
       "3  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...  intermediate  1209   \n",
       "4  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...  intermediate   751   \n",
       "5  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\in...  intermediate  3752   \n",
       "6  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\ra...           raw  3955   \n",
       "7  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\ra...           raw  3913   \n",
       "8  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\ra...           raw    14   \n",
       "9  c:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\data\\ra...           raw    14   \n",
       "\n",
       "                                             columns  frequency   date_min  \\\n",
       "0  date,spread_2y_bps,spread_5y_bps,spread_10y_bp...      daily 2010-01-04   \n",
       "1                date,bid_ask_spread,pubout,n_issues    monthly 1980-01-31   \n",
       "2                                    date,fed_assets     weekly 2002-12-18   \n",
       "3                         date,fed_treasury_holdings     weekly 2002-12-18   \n",
       "4                              date,sofr,sofr_volume      daily 2019-01-02   \n",
       "5  date,spread_2y_bps,spread_5y_bps,spread_10y_bp...      daily 2010-01-04   \n",
       "6           Date,AUD,CAD,CHF,EUR,GBP,JPY,NZD,SEK,USD    unknown        NaT   \n",
       "7  ('SPX Index', 'PX_LAST'),('SPX Index', 'IDX_ES...    unknown        NaT   \n",
       "8  report_date,total_assets,total_reserves,total_...  quarterly        NaT   \n",
       "9  report_date,total_assets,total_reserves,total_...  quarterly        NaT   \n",
       "\n",
       "    date_max  key_columns                                         join_hints  \n",
       "0 2024-12-31         date        daily:date | keys:date | layer:intermediate  \n",
       "1 2014-12-31         date                     keys:date | layer:intermediate  \n",
       "2 2026-02-11         date       weekly:date | keys:date | layer:intermediate  \n",
       "3 2026-02-11         date       weekly:date | keys:date | layer:intermediate  \n",
       "4 2021-12-31         date        daily:date | keys:date | layer:intermediate  \n",
       "5 2024-12-31         date        daily:date | keys:date | layer:intermediate  \n",
       "6        NaT                                                       layer:raw  \n",
       "7        NaT                                                       layer:raw  \n",
       "8        NaT  report_date  quarterly:report_date | keys:report_date | lay...  \n",
       "9        NaT  report_date  quarterly:report_date | keys:report_date | lay...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog = build_data_catalog(repo_root / \"data\")\n",
    "catalog.to_csv(run_dir / \"data\" / \"data_catalog.csv\", index=False)\n",
    "catalog.to_parquet(run_dir / \"data\" / \"data_catalog.parquet\", index=False)\n",
    "catalog.to_markdown(run_dir / \"data\" / \"data_catalog.md\", index=False)\n",
    "catalog.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb58189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        date       strategy series  tenor      y_bps  treasury_based  \\\n",
       " 0 2010-01-04  TIPS_Treasury  arb_2    2.0  43.534609               1   \n",
       " 1 2010-01-05  TIPS_Treasury  arb_2    2.0  39.490811               1   \n",
       " 2 2010-01-06  TIPS_Treasury  arb_2    2.0  38.344764               1   \n",
       " 3 2010-01-07  TIPS_Treasury  arb_2    2.0  30.474542               1   \n",
       " 4 2010-01-08  TIPS_Treasury  arb_2    2.0  40.810449               1   \n",
       " \n",
       "    y_abs_bps          y  \n",
       " 0  43.534609  43.534609  \n",
       " 1  39.490811  39.490811  \n",
       " 2  38.344764  38.344764  \n",
       " 3  30.474542  30.474542  \n",
       " 4  40.810449  40.810449  ,\n",
       " {'series_count': 20,\n",
       "  'strategy_count': 4,\n",
       "  'median_abs_bps': 22.159000000000013,\n",
       "  'p90_abs_bps': 63.76055393562226,\n",
       "  'p99_abs_bps': 197.69811111111116,\n",
       "  'y_col': 'y_abs_bps'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outcomes: build stacked arbitrage panel (TIPS–Treasury, Treasury SF, CIP, Equity SF)\n",
    "from slr_bucket.outcomes import stack_outcomes\n",
    "logger = logging.getLogger(\"summary_pipeline\")\n",
    "\n",
    "series_dir = repo_root / \"data\" / \"series\"\n",
    "outcomes_long = stack_outcomes(series_dir)\n",
    "\n",
    "# Canonical outcome used in regressions\n",
    "y_col = CONFIG[\"y_col\"]\n",
    "outcomes_long[\"y\"] = outcomes_long[y_col].astype(float)\n",
    "\n",
    "# A stable identifier used for FE and stratification\n",
    "outcomes_long[\"series\"] = outcomes_long[\"series\"].astype(str)\n",
    "outcomes_long[\"strategy\"] = outcomes_long[\"strategy\"].astype(str)\n",
    "outcomes_long[\"treasury_based\"] = outcomes_long[\"treasury_based\"].astype(int)\n",
    "\n",
    "# quick unit check\n",
    "unit_q = outcomes_long[\"y\"].abs().quantile([0.5, 0.9, 0.99]).to_dict()\n",
    "info = {\n",
    "    \"series_count\": int(outcomes_long[\"series\"].nunique()),\n",
    "    \"strategy_count\": int(outcomes_long[\"strategy\"].nunique()),\n",
    "    \"median_abs_bps\": float(unit_q.get(0.5, np.nan)),\n",
    "    \"p90_abs_bps\": float(unit_q.get(0.9, np.nan)),\n",
    "    \"p99_abs_bps\": float(unit_q.get(0.99, np.nan)),\n",
    "    \"y_col\": y_col,\n",
    "}\n",
    "(outcomes_long.head(), info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d72560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls: prefer intermediate analysis_panel if valid, else fallback build from raw.\n",
    "# def build_controls_panel():\n",
    "\n",
    "needed = set(CONFIG[\"direct_controls\"])\n",
    "try:\n",
    "    p = resolve_dataset_path(\"analysis_panel\", expected_dir=repo_root / \"data\" / \"intermediate\")\n",
    "    panel = load_any_table(p)\n",
    "    panel[\"date\"] = pd.to_datetime(panel[\"date\"], errors=\"coerce\")\n",
    "    if needed.issubset(set(panel.columns)):\n",
    "        logger.info(\"Using controls from intermediate analysis_panel: %s\", p)\n",
    "        controls =  panel[[\"date\", *sorted(needed)]].copy() #, str(p)\n",
    "except Exception as exc:\n",
    "    logger.warning(\"analysis_panel unavailable/invalid (%s), using raw fallback\", exc)\n",
    "\n",
    "fred = load_any_table(resolve_dataset_path(\"controls_vix_creditspreads_fred\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "fred[\"date\"] = pd.to_datetime(fred[\"date\"], errors=\"coerce\")\n",
    "fred[\"date\"] = as_daily_date(fred[\"date\"])\n",
    "try:\n",
    "    repo = load_any_table(resolve_dataset_path(\"repo_rates_combined\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "except FileNotFoundError:\n",
    "    repo = load_any_table(resolve_dataset_path(\"repo_rates_fred\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "repo[\"date\"] = pd.to_datetime(repo[\"date\"], errors=\"coerce\")\n",
    "repo[\"date\"] = as_daily_date(repo[\"date\"])\n",
    "repo = repo.rename(columns={\"TGCR\":\"tgcr\", \"EFFR\":\"effr\"})\n",
    "if \"spr_tgcr\" not in repo.columns and {\"SOFR\",\"tgcr\"}.issubset(repo.columns):\n",
    "    repo[\"spr_tgcr\"] = pd.to_numeric(repo[\"tgcr\"], errors=\"coerce\") - pd.to_numeric(repo[\"SOFR\"], errors=\"coerce\")\n",
    "if \"spr_effr\" not in repo.columns and {\"SOFR\",\"effr\"}.issubset(repo.columns):\n",
    "    repo[\"spr_effr\"] = pd.to_numeric(repo[\"effr\"], errors=\"coerce\") - pd.to_numeric(repo[\"SOFR\"], errors=\"coerce\")\n",
    "\n",
    "issu = load_any_table(resolve_dataset_path(\"treasury_issuance_by_tenor_fiscaldata\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "issu[\"date\"] = pd.to_datetime(issu.get(\"issue_date\"), errors=\"coerce\")\n",
    "issu[\"date\"] = as_daily_date(issu[\"date\"])\n",
    "issu[\"tenor_bucket\"] = pd.to_numeric(issu[\"tenor_bucket\"], errors=\"coerce\")\n",
    "issu[\"issuance_amount\"] = pd.to_numeric(issu[\"issuance_amount\"], errors=\"coerce\") / 1e9\n",
    "d = issu.pivot_table(index=\"date\", columns=\"tenor_bucket\", values=\"issuance_amount\", aggfunc=\"sum\").reset_index()\n",
    "\n",
    "# Robustly rename tenor-bucket columns to issu_*_bil (handles int/float/str column labels)\n",
    "rename_map = {}\n",
    "for col in d.columns:\n",
    "    if col == \"date\":\n",
    "        continue\n",
    "    try:\n",
    "        v = float(col)\n",
    "    except Exception:\n",
    "        continue\n",
    "    if abs(v - 7.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_7_bil\"\n",
    "    elif abs(v - 10.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_10_bil\"\n",
    "    elif abs(v - 14.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_14_bil\"\n",
    "    elif abs(v - 20.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_20_bil\"\n",
    "    elif abs(v - 30.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_30_bil\"\n",
    "d = d.rename(columns=rename_map)\n",
    "\n",
    "# Ensure required issuance controls exist (zeros if not present in file)\n",
    "for c in [\"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\", \"issu_10_bil\", \"issu_20_bil\"]:\n",
    "    if c not in d.columns:\n",
    "        d[c] = 0.0\n",
    "\n",
    "# If 14y bucket absent, approximate as 10y+20y (as in prior logic)\n",
    "if d[\"issu_14_bil\"].fillna(0.0).abs().sum() == 0.0:\n",
    "    d[\"issu_14_bil\"] = d.get(\"issu_10_bil\", 0.0) + d.get(\"issu_20_bil\", 0.0)\n",
    "\n",
    "for c in [\"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"]:\n",
    "    d[c] = pd.to_numeric(d[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# Keep only the issuance controls used in the design\n",
    "d = d[[\"date\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"]]\n",
    "fred = fred.groupby(\"date\", as_index=False).mean(numeric_only=True)\n",
    "repo = repo.groupby(\"date\", as_index=False).mean(numeric_only=True)\n",
    "d    = d.groupby(\"date\", as_index=False).sum(numeric_only=True)   # issuance is additive\n",
    "\n",
    "for col in [\"VIX\",\"HY_OAS\",\"BAA10Y\",\"SOFR\",\"spr_tgcr\",\"spr_effr\",\"tgcr\",\"effr\"]:\n",
    "    if col in fred.columns: fred[col] = coerce_num(fred[col])\n",
    "    if col in repo.columns: repo[col] = coerce_num(repo[col])\n",
    "\n",
    "\n",
    "# If 'controls' was not set from intermediate analysis_panel, build it from raw sources.\n",
    "if \"controls\" not in globals():\n",
    "    controls = fred.merge(repo, on=\"date\", how=\"outer\").merge(d, on=\"date\", how=\"outer\").sort_values(\"date\")\n",
    "    # keep only needed controls (drop extras like tgcr/effr if not needed)\n",
    "    keep = [\"date\"] + sorted(set(CONFIG[\"direct_controls\"]) & set(controls.columns))\n",
    "    controls = controls[keep].copy()\n",
    "    logger.info(\"Built controls from raw sources. columns=%s\", keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b5e541",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arb_long' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.groupby(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m, as_index=\u001b[38;5;28;01mFalse\u001b[39;00m)[num].agg(agg).sort_values(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 0) Build a master date index from your OUTCOMES (arb panel)\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# assumes you have arb_long/panel with a 'date' column\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m base_dates = _as_date(\u001b[43marb_long\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m]).dropna().drop_duplicates().sort_values()\n\u001b[32m     47\u001b[39m controls = pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m: base_dates}).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# 1) FRED controls\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'arb_long' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _as_date(x):\n",
    "    return pd.to_datetime(x, errors=\"coerce\", utc=True).dt.tz_convert(None).dt.normalize()\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def _canon(x) -> str:\n",
    "    # robust to non-string column names (float/int/None)\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x)\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", s.lower())\n",
    "\n",
    "def _sanitize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c) for c in df.columns]  # force all to strings\n",
    "    return df\n",
    "\n",
    "def _rename_to_canonical(df: pd.DataFrame, want: list[str]) -> pd.DataFrame:\n",
    "    df = _sanitize_columns(df)\n",
    "    m = {_canon(c): c for c in df.columns}\n",
    "    ren = {}\n",
    "    for w in want:\n",
    "        key = _canon(w)\n",
    "        if key in m:\n",
    "            ren[m[key]] = w\n",
    "    return df.rename(columns=ren)\n",
    "\n",
    "\n",
    "def _collapse_daily(df: pd.DataFrame, how=\"mean\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = _as_date(df[\"date\"])\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "    num = [c for c in df.columns if c != \"date\"]\n",
    "    if not num:\n",
    "        return df[[\"date\"]].drop_duplicates().sort_values(\"date\")\n",
    "    agg = \"mean\" if how == \"mean\" else \"sum\"\n",
    "    return df.groupby(\"date\", as_index=False)[num].agg(agg).sort_values(\"date\")\n",
    "\n",
    "# 0) Build a master date index from your OUTCOMES (arb panel)\n",
    "# assumes you have arb_long/panel with a 'date' column\n",
    "base_dates = _as_date(arb_long[\"date\"]).dropna().drop_duplicates().sort_values()\n",
    "controls = pd.DataFrame({\"date\": base_dates}).reset_index(drop=True)\n",
    "\n",
    "# 1) FRED controls\n",
    "fred_use = _rename_to_canonical(fred, [\"date\", \"VIX\", \"HY_OAS\", \"BAA10Y\"])\n",
    "fred_use = _collapse_daily(fred_use[[\"date\"] + [c for c in [\"VIX\",\"HY_OAS\",\"BAA10Y\"] if c in fred_use.columns]], how=\"mean\")\n",
    "controls = controls.merge(fred_use, on=\"date\", how=\"left\")\n",
    "\n",
    "# 2) Repo / funding controls\n",
    "repo_use = _rename_to_canonical(repo, [\"date\", \"SOFR\", \"TGCR\", \"EFFR\", \"spr_tgcr\", \"spr_effr\"])\n",
    "repo_use = _collapse_daily(repo_use, how=\"mean\")\n",
    "\n",
    "# if spreads missing but levels exist, compute them\n",
    "if \"spr_tgcr\" not in repo_use.columns and {\"TGCR\",\"SOFR\"}.issubset(repo_use.columns):\n",
    "    repo_use[\"spr_tgcr\"] = repo_use[\"TGCR\"] - repo_use[\"SOFR\"]\n",
    "if \"spr_effr\" not in repo_use.columns and {\"EFFR\",\"SOFR\"}.issubset(repo_use.columns):\n",
    "    repo_use[\"spr_effr\"] = repo_use[\"EFFR\"] - repo_use[\"SOFR\"]\n",
    "\n",
    "repo_keep = [\"date\"] + [c for c in [\"SOFR\",\"spr_tgcr\",\"spr_effr\"] if c in repo_use.columns]\n",
    "controls = controls.merge(repo_use[repo_keep], on=\"date\", how=\"left\")\n",
    "\n",
    "# 3) Issuance (event-based -> zeros on non-issuance days)\n",
    "d_use = d.copy()\n",
    "if \"issue_date\" in d_use.columns and \"date\" not in d_use.columns:\n",
    "    d_use = d_use.rename(columns={\"issue_date\": \"date\"})\n",
    "d_use = _rename_to_canonical(d_use, [\"date\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"])\n",
    "issu_use = _collapse_daily(d_use[[\"date\"] + [c for c in [\"issu_7_bil\",\"issu_14_bil\",\"issu_30_bil\"] if c in d_use.columns]], how=\"sum\")\n",
    "controls = controls.merge(issu_use, on=\"date\", how=\"left\")\n",
    "\n",
    "for c in [\"issu_7_bil\",\"issu_14_bil\",\"issu_30_bil\"]:\n",
    "    if c in controls.columns:\n",
    "        controls[c] = controls[c].fillna(0.0)\n",
    "\n",
    "# 4) Fill only market/funding gaps inside the outcome sample\n",
    "fill_cols = [c for c in [\"VIX\",\"HY_OAS\",\"BAA10Y\",\"SOFR\",\"spr_tgcr\",\"spr_effr\"] if c in controls.columns]\n",
    "controls = controls.sort_values(\"date\").reset_index(drop=True)\n",
    "controls[fill_cols] = controls[fill_cols].ffill().bfill()\n",
    "\n",
    "# 5) Quick diagnostic (within outcome sample only)\n",
    "miss = controls[fill_cols + [c for c in [\"issu_7_bil\",\"issu_14_bil\",\"issu_30_bil\"] if c in controls.columns]].isna().mean().sort_values(ascending=False)\n",
    "display(miss.to_frame(\"missing_share\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73532c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge outcomes with controls\n",
    "panel_long = outcomes_long.merge(controls, on=\"date\", how=\"left\")\n",
    "\n",
    "# coverage diagnostics for DIRECT controls (including missing/absent)\n",
    "need = CONFIG[\"direct_controls\"]\n",
    "present = [c for c in need if c in panel_long.columns]\n",
    "miss_share = panel_long[present].isna().mean().to_frame(\"missing_share\") if present else pd.DataFrame({\"missing_share\":[]})\n",
    "display(miss_share)\n",
    "\n",
    "print(panel_long.columns)\n",
    "\n",
    "# Preview\n",
    "panel_long.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics: missingness (including columns that are entirely absent)\n",
    "def missingness_report(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            out.append({\"var\": c, \"present\": False, \"missing_share\": 1.0, \"n_nonmissing\": 0})\n",
    "        else:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            out.append({\"var\": c, \"present\": True, \"missing_share\": float(s.isna().mean()), \"n_nonmissing\": int(s.notna().sum())})\n",
    "    return pd.DataFrame(out).sort_values([\"present\",\"missing_share\"], ascending=[True, False])\n",
    "\n",
    "diag = missingness_report(panel_long, CONFIG[\"direct_controls\"])\n",
    "display(diag)\n",
    "\n",
    "# Warn if any direct controls have less than 90% coverage in the sample used for event studies\n",
    "coverage = diag.set_index(\"var\")[\"missing_share\"]\n",
    "bad = coverage[coverage > 0.10]\n",
    "if len(bad):\n",
    "    logger.warning(\"Low coverage direct controls (>10%% missing): %s\", bad.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1A: summary stats by strategy and regime (using y = CONFIG['y_col'])\n",
    "regimes = {\n",
    "    \"pre\": (pd.Timestamp(\"2019-01-01\"), pd.Timestamp(\"2020-03-31\")),\n",
    "    \"relief\": (pd.Timestamp(\"2020-04-01\"), pd.Timestamp(\"2021-03-31\")),\n",
    "    \"post\": (pd.Timestamp(\"2021-04-01\"), pd.Timestamp.max),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for (strategy, series), g in panel_long.groupby([\"strategy\",\"series\"]):\n",
    "    g = g.sort_values(\"date\").set_index(\"date\")\n",
    "    for regime, (start, end) in regimes.items():\n",
    "        s = g.loc[(g.index>=start) & (g.index<=end), \"y\"].dropna()\n",
    "        if s.empty:\n",
    "            continue\n",
    "        lb_p = np.nan\n",
    "        try:\n",
    "            lb = acorr_ljungbox(s, lags=[min(10, max(1, len(s)//5))], return_df=True)\n",
    "            lb_p = float(lb[\"lb_pvalue\"].iloc[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "        rows.append({\n",
    "            \"strategy\": strategy,\n",
    "            \"series\": series,\n",
    "            \"treasury_based\": int(g[\"treasury_based\"].iloc[0]) if \"treasury_based\" in g.columns else np.nan,\n",
    "            \"regime\": regime,\n",
    "            \"sample_start\": s.index.min(),\n",
    "            \"sample_end\": s.index.max(),\n",
    "            \"N\": int(s.shape[0]),\n",
    "            \"mean\": float(s.mean()),\n",
    "            \"std\": float(s.std()),\n",
    "            \"p5\": float(s.quantile(0.05)),\n",
    "            \"p50\": float(s.quantile(0.50)),\n",
    "            \"p95\": float(s.quantile(0.95)),\n",
    "            \"autocorr1\": float(s.autocorr(lag=1)) if len(s) > 2 else np.nan,\n",
    "            \"ljungbox_pvalue\": lb_p,\n",
    "        })\n",
    "summary_stats = pd.DataFrame(rows)\n",
    "summary_stats.to_csv(run_dir / \"tables\" / \"summary_stats_by_strategy.csv\", index=False)\n",
    "summary_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d695467",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_long = panel_long.reset_index(drop=True)\n",
    "if \"date\" not in panel_long.columns and isinstance(panel_long.index, pd.DatetimeIndex):\n",
    "    panel_long[\"date\"] = panel_long.index\n",
    "\n",
    "panel_long[\"date\"] = pd.to_datetime(panel_long[\"date\"], errors=\"coerce\")\n",
    "panel_long = panel_long.dropna(subset=[\"date\"]).sort_values([\"tenor\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1B: jump regressions (series-level TOTAL vs DIRECT) + pooled group interaction\n",
    "from slr_bucket.econometrics.event_study import pooled_jump_regression\n",
    "\n",
    "jump_rows = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for w in CONFIG[\"windows\"]:\n",
    "        for (strategy, series), g in panel_long.groupby([\"strategy\",\"series\"]):\n",
    "            for spec, controls_list in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "                est, se, n = jump_estimator(g, y_col=\"y\", event_date=event, window=w, controls=controls_list, hac_lags=CONFIG[\"hac_lags\"])\n",
    "                jump_rows.append({\n",
    "                    \"event\": event, \"window\": w, \"strategy\": strategy, \"series\": series,\n",
    "                    \"treasury_based\": int(g[\"treasury_based\"].iloc[0]),\n",
    "                    \"spec\": spec, \"estimate\": est, \"se\": se,\n",
    "                    \"ci_low\": est - 1.96*se if pd.notna(est) and pd.notna(se) else np.nan,\n",
    "                    \"ci_high\": est + 1.96*se if pd.notna(est) and pd.notna(se) else np.nan,\n",
    "                    \"N\": n,\n",
    "                })\n",
    "jump_results = pd.DataFrame(jump_rows)\n",
    "jump_results.to_csv(run_dir / \"tables\" / \"jump_results_by_series.csv\", index=False)\n",
    "\n",
    "# Pooled jump with group interaction (Treasury-based vs non) using series FE\n",
    "pooled_rows = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    work = panel_long.copy()\n",
    "    for w in CONFIG[\"windows\"]:\n",
    "        for spec, controls_list in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "            out = pooled_jump_regression(\n",
    "                work, y_col=\"y\", event_date=event, window=w,\n",
    "                group_col=CONFIG[\"group_col\"], fe_col=CONFIG[\"fe_col\"],\n",
    "                controls=controls_list, hac_lags=CONFIG[\"hac_lags\"],\n",
    "            )\n",
    "            if out.empty:\n",
    "                continue\n",
    "            out[\"event\"] = event\n",
    "            out[\"window\"] = w\n",
    "            out[\"spec\"] = spec\n",
    "            pooled_rows.append(out)\n",
    "pooled_jump = pd.concat(pooled_rows, ignore_index=True) if pooled_rows else pd.DataFrame()\n",
    "pooled_jump.to_csv(run_dir / \"tables\" / \"pooled_jump_group_interaction.csv\", index=False)\n",
    "\n",
    "(jump_results.head(), pooled_jump)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1C: binned event-study paths by strategy + pooled Treasury-based interactions\n",
    "from slr_bucket.econometrics.event_study import pooled_event_study\n",
    "from slr_bucket.plotting.plots import plot_binned_event_overlay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = CONFIG[\"event_bins\"]\n",
    "\n",
    "# (i) Event-time paths by STRATEGY (pooled within strategy, series FE; no group interaction)\n",
    "strategy_paths = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for strategy, g in panel_long.groupby(\"strategy\"):\n",
    "        g = g.copy()\n",
    "        g[\"__g0\"] = 0  # force no group interaction; pooled_event_study will drop constant columns\n",
    "        res_df, _ = pooled_event_study(\n",
    "            g, y_col=\"y\", event_date=event, bins=bins,\n",
    "            group_col=\"__g0\", fe_col=CONFIG[\"fe_col\"],\n",
    "            controls=CONFIG[\"direct_controls\"], hac_lags=CONFIG[\"hac_lags\"],\n",
    "        )\n",
    "        if res_df.empty:\n",
    "            continue\n",
    "        res_df[\"strategy\"] = strategy\n",
    "        strategy_paths.append(res_df)\n",
    "\n",
    "        # Save overlay plot for this strategy/event (plots group0_effect == pooled path)\n",
    "        plot_binned_event_overlay(\n",
    "            res_df[res_df[\"kind\"].isin([\"group0_effect\"])].copy(),\n",
    "            title=f\"Event-study path | {strategy} | event={event}\",\n",
    "            outpath=run_dir / \"figures\" / f\"event_path_{strategy}_{event}.png\",\n",
    "        )\n",
    "\n",
    "strategy_paths_df = pd.concat(strategy_paths, ignore_index=True) if strategy_paths else pd.DataFrame()\n",
    "strategy_paths_df.to_csv(run_dir / \"tables\" / \"eventstudy_paths_by_strategy.csv\", index=False)\n",
    "\n",
    "# (ii) Pooled Treasury-based vs non interactions across ALL strategies (series FE)\n",
    "pooled_all = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    res_df, _ = pooled_event_study(\n",
    "        panel_long, y_col=\"y\", event_date=event, bins=bins,\n",
    "        group_col=CONFIG[\"group_col\"], fe_col=CONFIG[\"fe_col\"],\n",
    "        controls=CONFIG[\"direct_controls\"], hac_lags=CONFIG[\"hac_lags\"],\n",
    "    )\n",
    "    if res_df.empty:\n",
    "        continue\n",
    "    pooled_all.append(res_df)\n",
    "\n",
    "pooled_all_df = pd.concat(pooled_all, ignore_index=True) if pooled_all else pd.DataFrame()\n",
    "pooled_all_df.to_csv(run_dir / \"tables\" / \"eventstudy_pooled_treasury_interactions.csv\", index=False)\n",
    "\n",
    "# Plot the pooled comparison (group0 vs group1 + interaction bins)\n",
    "for event in CONFIG[\"events\"]:\n",
    "    sub = pooled_all_df[pooled_all_df[\"event_date\"] == event].copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    plot_binned_event_overlay(\n",
    "        sub[sub[\"kind\"].isin([\"group0_effect\",\"group1_effect\",\"interaction_bin\"])].copy(),\n",
    "        title=f\"Pooled event-study | Treasury-based vs non | event={event}\",\n",
    "        outpath=run_dir / \"figures\" / f\"event_path_pooled_treasury_vs_non_{event}.png\",\n",
    "    )\n",
    "\n",
    "(strategy_paths_df.head(), pooled_all_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check: plot raw series around each event (per SERIES), with event date highlighted\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_event_window_series(df: pd.DataFrame, series: str, event: str, window: int = 120):\n",
    "    d = df[df[\"series\"] == series].copy()\n",
    "    d = d.sort_values(\"date\").set_index(\"date\")\n",
    "    t0 = pd.Timestamp(event)\n",
    "    sub = d.loc[(d.index >= t0 - pd.Timedelta(days=window)) & (d.index <= t0 + pd.Timedelta(days=window)), [\"y\"]].dropna()\n",
    "    if sub.empty:\n",
    "        return\n",
    "    fig, ax = plt.subplots(figsize=(9,4))\n",
    "    ax.plot(sub.index, sub[\"y\"])\n",
    "    ax.axvline(t0, color=\"black\", ls=\"--\", lw=1)\n",
    "    ax.set_title(f\"{series} | raw y around {event} (±{window}d)\")\n",
    "    ax.set_ylabel(\"bps\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(run_dir / \"figures\" / f\"raw_{series}_window_{event}.png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for s in sorted(panel_long[\"series\"].dropna().unique().tolist()):\n",
    "        plot_event_window_series(panel_long, s, event, window=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1D: overlay strategy paths in a single figure (per event)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "paths = pd.read_csv(run_dir / \"tables\" / \"eventstudy_paths_by_strategy.csv\") if (run_dir / \"tables\" / \"eventstudy_paths_by_strategy.csv\").exists() else strategy_paths_df.copy()\n",
    "if not paths.empty:\n",
    "    # keep pooled path rows\n",
    "    paths = paths[paths[\"kind\"] == \"group0_effect\"].copy()\n",
    "    paths[\"bin_mid\"] = pd.to_numeric(paths[\"bin_mid\"], errors=\"coerce\")\n",
    "    for event in CONFIG[\"events\"]:\n",
    "        sub = paths[paths[\"event_date\"] == event].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        for strategy, g in sub.groupby(\"strategy\"):\n",
    "            g = g.sort_values(\"bin_mid\")\n",
    "            ax.plot(g[\"bin_mid\"], g[\"estimate\"], marker=\"o\", label=strategy)\n",
    "        ax.axhline(0, color=\"black\", lw=1)\n",
    "        ax.axvline(0, color=\"black\", lw=1, ls=\"--\")\n",
    "        ax.set_xlabel(\"Event time (bin midpoint)\")\n",
    "        ax.set_ylabel(\"Effect (bps)\")\n",
    "        ax.set_title(f\"Event-study paths by strategy | event={event}\")\n",
    "        ax.grid(alpha=0.2)\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(run_dir / \"figures\" / f\"event_paths_all_strategies_{event}.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "paths.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2 mechanism (weekly): pooled panel with series FE + interactions by treasury_based\n",
    "import statsmodels.api as sm\n",
    "layer2_note = \"\"\n",
    "mech_out = pd.DataFrame()\n",
    "try:\n",
    "    if not CONFIG.get(\"run_layer2\", True):\n",
    "        raise RuntimeError(\"CONFIG.run_layer2=False\")\n",
    "\n",
    "    # inputs\n",
    "    pd_long = load_any_table(resolve_dataset_path(\"primary_dealer_stats_ofr_stfm_nypd_long\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    bank = load_any_table(resolve_dataset_path(\"bank_exposure_y9c_agg_daily\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "\n",
    "    pd_long[\"date\"] = pd.to_datetime(pd_long[\"date\"], errors=\"coerce\")\n",
    "    bank[\"date\"] = pd.to_datetime(bank[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # Weekly dealer utilization proxy (lagged)\n",
    "    pd_w = pd_long.pivot_table(index=\"date\", columns=\"mnemonic\", values=\"value\", aggfunc=\"mean\").resample(\"W-FRI\").mean()\n",
    "    pd_w[\"utilization_index\"] = pd_w.sum(axis=1, min_count=1)\n",
    "    pd_w[\"utilization_lag1w\"] = pd_w[\"utilization_index\"].shift(1)\n",
    "\n",
    "    # Weekly bank exposure proxy\n",
    "    if \"agg_exempt_share\" not in bank.columns:\n",
    "        raise KeyError(\"bank_exposure_y9c_agg_daily missing 'agg_exempt_share'\")\n",
    "    b_w = bank.set_index(\"date\").resample(\"W-FRI\").mean()[[\"agg_exempt_share\"]]\n",
    "\n",
    "    # Weekly outcome per SERIES\n",
    "    y_w = panel_long.set_index(\"date\").groupby(\"series\")[\"y\"].resample(\"W-FRI\").mean().reset_index()\n",
    "    # attach strategy + treasury_based (time-invariant per series)\n",
    "    meta = panel_long.groupby(\"series\", as_index=False)[[\"strategy\",\"treasury_based\"]].first()\n",
    "    y_w = y_w.merge(meta, on=\"series\", how=\"left\").set_index(\"date\")\n",
    "\n",
    "    # Weekly controls (take what is available)\n",
    "    desired = list(CONFIG.get(\"direct_controls\", []))\n",
    "    present = [c for c in desired if c in panel_long.columns]\n",
    "    missing = sorted(set(desired) - set(present))\n",
    "    if missing:\n",
    "        logger.warning(\"Layer 2: dropping missing controls: %s\", missing)\n",
    "\n",
    "    c_w = panel_long.set_index(\"date\")[present].resample(\"W-FRI\").mean() if present else pd.DataFrame(index=y_w.index.unique())\n",
    "\n",
    "    # Merge: broadcast b_w and pd_w to all series dates\n",
    "    mech = y_w.join([b_w, pd_w[[\"utilization_lag1w\"]], c_w], how=\"inner\").dropna()\n",
    "\n",
    "    # Relief indicator (inclusive)\n",
    "    mech[\"relief\"] = ((mech.index >= \"2020-04-01\") & (mech.index <= \"2021-03-31\")).astype(int)\n",
    "\n",
    "    # z-scores\n",
    "    ex_std = mech[\"agg_exempt_share\"].std()\n",
    "    util_std = mech[\"utilization_lag1w\"].std()\n",
    "    mech[\"z_exempt\"] = (mech[\"agg_exempt_share\"] - mech[\"agg_exempt_share\"].mean()) / (ex_std if ex_std and ex_std > 0 else 1.0)\n",
    "    mech[\"z_util_l1\"] = (mech[\"utilization_lag1w\"] - mech[\"utilization_lag1w\"].mean()) / (util_std if util_std and util_std > 0 else 1.0)\n",
    "\n",
    "    mech[\"treasury_based\"] = pd.to_numeric(mech[\"treasury_based\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # interactions\n",
    "    mech[\"relief_x_exempt\"] = mech[\"relief\"] * mech[\"z_exempt\"]\n",
    "    mech[\"relief_x_util\"]   = mech[\"relief\"] * mech[\"z_util_l1\"]\n",
    "    mech[\"relief_x_treas\"]  = mech[\"relief\"] * mech[\"treasury_based\"]\n",
    "    mech[\"relief_x_exempt_x_treas\"] = mech[\"relief\"] * mech[\"z_exempt\"] * mech[\"treasury_based\"]\n",
    "    mech[\"relief_x_util_x_treas\"]   = mech[\"relief\"] * mech[\"z_util_l1\"] * mech[\"treasury_based\"]\n",
    "\n",
    "    # Series FE\n",
    "    fe = pd.get_dummies(mech[\"series\"].astype(str), prefix=\"fe\", drop_first=True)\n",
    "\n",
    "    xcols = [\"relief\", \"relief_x_exempt\", \"relief_x_util\",\n",
    "             \"relief_x_treas\", \"relief_x_exempt_x_treas\", \"relief_x_util_x_treas\"] + present\n",
    "    X = pd.concat([mech[xcols].apply(pd.to_numeric, errors=\"coerce\"), fe], axis=1)\n",
    "    reg = pd.concat([mech[[\"y\"]], X], axis=1).dropna()\n",
    "\n",
    "    if len(reg) < 50:\n",
    "        raise RuntimeError(f\"Layer 2 insufficient weekly observations after joins: n={len(reg)}\")\n",
    "\n",
    "    Y = reg[\"y\"].astype(float)\n",
    "    X = sm.add_constant(reg.drop(columns=[\"y\"]).astype(float), has_constant=\"add\")\n",
    "    res = sm.OLS(Y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\":2})\n",
    "\n",
    "    mech_out = pd.DataFrame({\"term\": res.params.index, \"coef\": res.params.values, \"se\": res.bse.values})\n",
    "    mech_out.to_csv(run_dir / \"tables\" / \"layer2_mechanism_weekly_panel.csv\", index=False)\n",
    "\n",
    "    layer2_note = f\"Layer 2 executed successfully. n={int(res.nobs)}; controls_used={present}; series_FE={fe.shape[1]}\"\n",
    "except Exception as exc:\n",
    "    layer2_note = f\"Layer 2 skipped gracefully due to missing/unusable inputs: {exc}\"\n",
    "\n",
    "print(layer2_note)\n",
    "mech_out.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_w.head() if 'pd_w' in globals() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b931e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_w.head() if 'y_w' in globals() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f149a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mech_out if 'mech_out' in globals() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run dir: C:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\outputs\\summary_pipeline\\20260226_221836_278369efc0\n",
      "Latest refreshed: C:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\outputs\\summary_pipeline\\latest\n",
      "Config hash: 278369efc0\n"
     ]
    }
   ],
   "source": [
    "# Refresh latest and write run metadata (works when CONFIG is a dict)\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import shutil\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def _safe_git_info(repo_root: Path) -> dict:\n",
    "    try:\n",
    "        sha = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"HEAD\"], cwd=str(repo_root), stderr=subprocess.STDOUT, text=True\n",
    "        ).strip()\n",
    "        branch = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=str(repo_root), stderr=subprocess.STDOUT, text=True\n",
    "        ).strip()\n",
    "        status = subprocess.check_output(\n",
    "            [\"git\", \"status\", \"--porcelain\"], cwd=str(repo_root), stderr=subprocess.STDOUT, text=True\n",
    "        )\n",
    "        return {\"commit\": sha, \"branch\": branch, \"dirty\": bool(status.strip())}\n",
    "    except Exception as e:\n",
    "        return {\"commit\": None, \"branch\": None, \"dirty\": None, \"error\": str(e)}\n",
    "\n",
    "def _safe_pkg_versions(pkgs: list[str]) -> dict:\n",
    "    out = {}\n",
    "    try:\n",
    "        import importlib.metadata as md\n",
    "        for p in pkgs:\n",
    "            try:\n",
    "                out[p] = md.version(p)\n",
    "            except Exception:\n",
    "                out[p] = None\n",
    "    except Exception:\n",
    "        for p in pkgs:\n",
    "            out[p] = None\n",
    "    return out\n",
    "\n",
    "def _find_main_df() -> tuple[str | None, pd.DataFrame | None]:\n",
    "    for name in [\"analysis_panel\", \"arb_panel\", \"panel\", \"daily_long\", \"pivot\"]:\n",
    "        obj = globals().get(name, None)\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            return name, obj\n",
    "    return None, None\n",
    "\n",
    "def _dataset_notes() -> str:\n",
    "    name, df = _find_main_df()\n",
    "    if df is None:\n",
    "        return \"No main dataframe found (looked for: analysis_panel, arb_panel, panel, daily_long, pivot).\"\n",
    "    notes = [f\"Dataset: {name}\", f\"Rows: {len(df):,}\", f\"Columns: {df.shape[1]:,}\"]\n",
    "    if \"date\" in df.columns:\n",
    "        d = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        if d.notna().any():\n",
    "            notes.append(f\"Date range: {d.min().date()} → {d.max().date()}\")\n",
    "    if \"tenor\" in df.columns:\n",
    "        try:\n",
    "            notes.append(f\"Tenors: {df['tenor'].nunique()}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return \"\\n\".join(notes)\n",
    "\n",
    "def _stable_config_dict(cfg) -> dict:\n",
    "    # If cfg is already dict-like, use it; else try to coerce.\n",
    "    if isinstance(cfg, dict):\n",
    "        return cfg\n",
    "    try:\n",
    "        return dict(cfg)\n",
    "    except Exception:\n",
    "        return {\"CONFIG_repr\": repr(cfg)}\n",
    "\n",
    "def _config_hash(cfg_dict: dict) -> str:\n",
    "    s = json.dumps(cfg_dict, sort_keys=True, default=str).encode(\"utf-8\")\n",
    "    return hashlib.sha1(s).hexdigest()[:10]\n",
    "\n",
    "def _write_readme(run_dir: Path, cfg_dict: dict, notes: str):\n",
    "    txt = []\n",
    "    txt.append(\"# summary_pipeline run\")\n",
    "    txt.append(\"\")\n",
    "    txt.append(\"## Config\")\n",
    "    txt.append(\"```json\")\n",
    "    txt.append(json.dumps(cfg_dict, indent=2, default=str))\n",
    "    txt.append(\"```\")\n",
    "    txt.append(\"\")\n",
    "    txt.append(\"## Notes\")\n",
    "    txt.append(\"```\")\n",
    "    txt.append(notes)\n",
    "    txt.append(\"```\")\n",
    "    (run_dir / \"README.md\").write_text(\"\\n\".join(txt), encoding=\"utf-8\")\n",
    "\n",
    "# --- Preconditions\n",
    "if \"CONFIG\" not in globals():\n",
    "    raise RuntimeError(\"CONFIG not found. Run the CONFIG cell first.\")\n",
    "\n",
    "# Repo root (best-effort)\n",
    "if \"REPO_ROOT\" in globals():\n",
    "    repo_root = Path(REPO_ROOT)\n",
    "else:\n",
    "    repo_root = Path().cwd().resolve().parent  # assumes notebook is in notebooks/\n",
    "\n",
    "# Output root (prefer CONFIG override if provided)\n",
    "cfg_dict = _stable_config_dict(CONFIG)\n",
    "out_root = cfg_dict.get(\"output_root\", None)\n",
    "if out_root:\n",
    "    output_root = Path(out_root).expanduser()\n",
    "    if not output_root.is_absolute():\n",
    "        output_root = (repo_root / output_root).resolve()\n",
    "else:\n",
    "    output_root = (repo_root / \"outputs\" / \"summary_pipeline\").resolve()\n",
    "\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run folder naming\n",
    "utc_now = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "cfg_hash = _config_hash(cfg_dict)\n",
    "\n",
    "run_dir = output_root / f\"{ts}_{cfg_hash}\"\n",
    "figures_dir = run_dir / \"figures\"\n",
    "tables_dir  = run_dir / \"tables\"\n",
    "data_dir    = run_dir / \"data\"\n",
    "logs_dir    = run_dir / \"logs\"\n",
    "\n",
    "for p in [run_dir, figures_dir, tables_dir, data_dir, logs_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write README + metadata\n",
    "notes = _dataset_notes()\n",
    "_write_readme(run_dir, cfg_dict, notes)\n",
    "\n",
    "metadata = {\n",
    "    \"utc_timestamp\": utc_now,\n",
    "    \"run_dir\": str(run_dir),\n",
    "    \"config_hash\": cfg_hash,\n",
    "    \"config\": cfg_dict,\n",
    "    \"git\": _safe_git_info(repo_root),\n",
    "    \"python\": {\"version\": sys.version, \"executable\": sys.executable},\n",
    "    \"platform\": {\n",
    "        \"system\": platform.system(),\n",
    "        \"release\": platform.release(),\n",
    "        \"version\": platform.version(),\n",
    "        \"machine\": platform.machine(),\n",
    "    },\n",
    "    \"packages\": _safe_pkg_versions([\"numpy\", \"pandas\", \"statsmodels\", \"matplotlib\", \"scipy\"]),\n",
    "    \"notes\": notes,\n",
    "}\n",
    "(run_dir / \"run_metadata.json\").write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Refresh latest/ (copy entire run folder)\n",
    "latest_dir = output_root / \"latest\"\n",
    "if latest_dir.exists():\n",
    "    shutil.rmtree(latest_dir)\n",
    "shutil.copytree(run_dir, latest_dir)\n",
    "\n",
    "# Expose run_dirs for downstream cells (compatible with older code)\n",
    "run_dirs = {\n",
    "    \"run\": run_dir,\n",
    "    \"figures\": figures_dir,\n",
    "    \"tables\": tables_dir,\n",
    "    \"data\": data_dir,\n",
    "    \"logs\": logs_dir,\n",
    "    \"latest\": latest_dir,\n",
    "}\n",
    "\n",
    "print(\"Run dir:\", run_dir)\n",
    "print(\"Latest refreshed:\", latest_dir)\n",
    "print(\"Config hash:\", cfg_hash)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
