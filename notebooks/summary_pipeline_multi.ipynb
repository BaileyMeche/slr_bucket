{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary pipeline (multi-tenor arb outcomes)\n",
    "This notebook runs the arb outcome event-study pipeline using real repo data only (no synthetic seeds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "from slr_bucket.econometrics.event_study import add_event_time, event_study_regression, jump_estimator\n",
    "from slr_bucket.io import build_data_catalog, load_any_table, resolve_dataset_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"outcomes_source\": \"tips_treasury_implied_rf_2010\",\n",
    "    \"outcome_pattern\": \"arb_\",\n",
    "    \"tenors_required\": [2, 5, 10],\n",
    "    \"events\": [\"2020-04-01\", \"2021-03-19\", \"2021-03-31\"],\n",
    "    \"windows\": [20, 60],\n",
    "    \"event_bins\": [(-60, -41), (-40, -21), (-20, -1), (0, 0), (1, 20), (21, 40), (41, 60)],\n",
    "    \"total_controls\": [\"VIX\", \"HY_OAS\", \"BAA10Y\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"],\n",
    "    \"direct_controls\": [\"VIX\", \"HY_OAS\", \"BAA10Y\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\", \"SOFR\", \"spr_tgcr\", \"spr_effr\"],\n",
    "    \"hac_lags\": 5,\n",
    "}\n",
    "repo_root = Path.cwd()\n",
    "cfg_hash = hashlib.sha256(json.dumps(CONFIG, sort_keys=True).encode()).hexdigest()[:12]\n",
    "run_stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = repo_root / \"outputs\" / \"summary_pipeline\" / f\"{run_stamp}_{cfg_hash}\"\n",
    "for sub in [\"figures\", \"tables\", \"data\", \"logs\"]:\n",
    "    (run_dir / sub).mkdir(parents=True, exist_ok=True)\n",
    "latest_dir = repo_root / \"outputs\" / \"summary_pipeline\" / \"latest\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\", handlers=[logging.FileHandler(run_dir / \"logs\" / \"pipeline.log\"), logging.StreamHandler()], force=True)\n",
    "logger = logging.getLogger(\"summary_pipeline_multi\")\n",
    "run_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data map for the new `/data` structure\n",
    "Used in this run:\n",
    "- Outcomes: `data/series/tips_treasury_implied_rf_2010.(parquet|csv)` (`arb_*` only).\n",
    "- Controls (preferred): `data/intermediate/analysis_panel.csv` if all required columns are present.\n",
    "- Controls (fallback):\n",
    "  - `data/raw/event_inputs/controls_vix_creditspreads_fred.(parquet|csv)`\n",
    "  - `data/raw/event_inputs/repo_rates_combined.(parquet|csv)` or `repo_rates_fred`\n",
    "  - `data/raw/event_inputs/treasury_issuance_by_tenor_fiscaldata.(parquet|csv)`\n",
    "- Mechanism proxies (optional):\n",
    "  - `primary_dealer_stats_ofr_stfm_nypd_long`\n",
    "  - `bank_exposure_y9c_agg_daily`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = build_data_catalog(repo_root / \"data\")\n",
    "catalog.to_csv(run_dir / \"data\" / \"data_catalog.csv\", index=False)\n",
    "catalog.to_parquet(run_dir / \"data\" / \"data_catalog.parquet\", index=False)\n",
    "catalog.to_markdown(run_dir / \"data\" / \"data_catalog.md\", index=False)\n",
    "catalog.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_path = resolve_dataset_path(CONFIG[\"outcomes_source\"], expected_dir=repo_root / \"data\" / \"series\")\n",
    "outcomes = load_any_table(outcome_path)\n",
    "outcomes[\"date\"] = pd.to_datetime(outcomes[\"date\"], errors=\"coerce\")\n",
    "arb_cols = sorted([c for c in outcomes.columns if c.startswith(\"arb_\")], key=lambda x: int(x.split(\"_\")[1]))\n",
    "if not arb_cols:\n",
    "    raise ValueError(\"No arb_* outcome columns found.\")\n",
    "arb_long = outcomes[[\"date\", *arb_cols]].melt(id_vars=[\"date\"], var_name=\"outcome\", value_name=\"y\")\n",
    "arb_long[\"tenor\"] = arb_long[\"outcome\"].str.extract(r\"arb_(\\d+)\").astype(float).astype(\"Int64\")\n",
    "arb_long[\"y\"] = pd.to_numeric(arb_long[\"y\"], errors=\"coerce\")\n",
    "arb_long = arb_long.dropna(subset=[\"date\", \"tenor\", \"y\"]).sort_values([\"tenor\", \"date\"]).reset_index(drop=True)\n",
    "value_q = arb_long[\"y\"].abs().quantile([0.5, 0.9, 0.99]).to_dict()\n",
    "unit_note = \"Likely basis points\" if value_q.get(0.5, 0) > 0.2 else \"Likely decimal units\"\n",
    "{\"outcome_path\": str(outcome_path), \"arb_cols\": arb_cols, \"value_q\": value_q, \"unit_note\": unit_note}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_controls() -> tuple[pd.DataFrame, str]:\n",
    "    required = set(CONFIG[\"direct_controls\"])\n",
    "    try:\n",
    "        panel_path = resolve_dataset_path(\"analysis_panel\", expected_dir=repo_root / \"data\" / \"intermediate\")\n",
    "        panel = load_any_table(panel_path)\n",
    "        panel[\"date\"] = pd.to_datetime(panel[\"date\"], errors=\"coerce\")\n",
    "        if required.issubset(panel.columns):\n",
    "            return panel[[\"date\", *sorted(required)]].copy(), str(panel_path)\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        logger.warning(\"analysis_panel not usable: %s\", exc)\n",
    "\n",
    "    controls = load_any_table(resolve_dataset_path(\"controls_vix_creditspreads_fred\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    controls[\"date\"] = pd.to_datetime(controls[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    try:\n",
    "        repo = load_any_table(resolve_dataset_path(\"repo_rates_combined\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    except FileNotFoundError:\n",
    "        repo = load_any_table(resolve_dataset_path(\"repo_rates_fred\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    repo[\"date\"] = pd.to_datetime(repo[\"date\"], errors=\"coerce\")\n",
    "    if \"spr_tgcr\" not in repo.columns and {\"SOFR\", \"TGCR\"}.issubset(repo.columns):\n",
    "        repo[\"spr_tgcr\"] = pd.to_numeric(repo[\"SOFR\"], errors=\"coerce\") - pd.to_numeric(repo[\"TGCR\"], errors=\"coerce\")\n",
    "    if \"spr_effr\" not in repo.columns and {\"SOFR\", \"EFFR\"}.issubset(repo.columns):\n",
    "        repo[\"spr_effr\"] = pd.to_numeric(repo[\"SOFR\"], errors=\"coerce\") - pd.to_numeric(repo[\"EFFR\"], errors=\"coerce\")\n",
    "\n",
    "    issuance = load_any_table(resolve_dataset_path(\"treasury_issuance_by_tenor_fiscaldata\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    issuance[\"date\"] = pd.to_datetime(issuance[\"issue_date\"], errors=\"coerce\")\n",
    "    issuance[\"tenor_bucket\"] = pd.to_numeric(issuance[\"tenor_bucket\"], errors=\"coerce\")\n",
    "    issuance[\"issuance_amount\"] = pd.to_numeric(issuance[\"issuance_amount\"], errors=\"coerce\") / 1e9\n",
    "    wide = issuance.pivot_table(index=\"date\", columns=\"tenor_bucket\", values=\"issuance_amount\", aggfunc=\"sum\").reset_index()\n",
    "    wide = wide.rename(columns={7.0: \"issu_7_bil\", 14.0: \"issu_14_bil\", 30.0: \"issu_30_bil\"})\n",
    "    for c in [\"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"]:\n",
    "        if c not in wide.columns:\n",
    "            wide[c] = np.nan\n",
    "\n",
    "    ctrl = controls.merge(repo[[c for c in [\"date\", \"SOFR\", \"spr_tgcr\", \"spr_effr\"] if c in repo.columns]], on=\"date\", how=\"outer\")\n",
    "    ctrl = ctrl.merge(wide[[\"date\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"]], on=\"date\", how=\"left\")\n",
    "    return ctrl, \"raw/event_inputs fallback\"\n",
    "\n",
    "controls, controls_source = _build_controls()\n",
    "controls.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_long = arb_long.merge(controls, on=\"date\", how=\"left\")\n",
    "for c in CONFIG[\"direct_controls\"]:\n",
    "    if c in panel_long.columns:\n",
    "        panel_long[c] = pd.to_numeric(panel_long[c], errors=\"coerce\")\n",
    "panel_long.to_parquet(run_dir / \"data\" / \"arb_panel_long.parquet\", index=False)\n",
    "panel_long.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regimes = {\n",
    "    \"pre\": (pd.Timestamp(\"2019-01-01\"), pd.Timestamp(\"2020-03-31\")),\n",
    "    \"relief\": (pd.Timestamp(\"2020-04-01\"), pd.Timestamp(\"2021-03-31\")),\n",
    "    \"post\": (pd.Timestamp(\"2021-04-01\"), pd.Timestamp.max),\n",
    "}\n",
    "rows = []\n",
    "for tenor, g in panel_long.groupby(\"tenor\"):\n",
    "    g = g.sort_values(\"date\")\n",
    "    for regime, (start, end) in regimes.items():\n",
    "        sub = g[(g[\"date\"] >= start) & (g[\"date\"] <= end)][[\"date\", \"y\"]].dropna()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        lb_p = np.nan\n",
    "        try:\n",
    "            lb = acorr_ljungbox(sub[\"y\"], lags=[min(10, max(1, len(sub) // 5))], return_df=True)\n",
    "            lb_p = float(lb[\"lb_pvalue\"].iloc[0])\n",
    "        except Exception:  # noqa: BLE001\n",
    "            pass\n",
    "        rows.append({\n",
    "            \"tenor\": int(tenor), \"regime\": regime,\n",
    "            \"sample_start\": sub[\"date\"].min(), \"sample_end\": sub[\"date\"].max(), \"N\": int(sub.shape[0]),\n",
    "            \"mean\": sub[\"y\"].mean(), \"std\": sub[\"y\"].std(),\n",
    "            \"p1\": sub[\"y\"].quantile(0.01), \"p5\": sub[\"y\"].quantile(0.05), \"p50\": sub[\"y\"].quantile(0.50), \"p95\": sub[\"y\"].quantile(0.95), \"p99\": sub[\"y\"].quantile(0.99),\n",
    "            \"autocorr1\": sub[\"y\"].autocorr(1), \"ljungbox_pvalue\": lb_p,\n",
    "        })\n",
    "summary_stats = pd.DataFrame(rows)\n",
    "summary_stats.to_csv(run_dir / \"tables\" / \"summary_stats.csv\", index=False)\n",
    "summary_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jump_rows = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for window in CONFIG[\"windows\"]:\n",
    "        for tenor, g in panel_long.groupby(\"tenor\"):\n",
    "            for spec, controls_set in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "                est, se, n = jump_estimator(g, y_col=\"y\", event_date=event, window=window, controls=controls_set, hac_lags=CONFIG[\"hac_lags\"])\n",
    "                jump_rows.append({\n",
    "                    \"event\": event, \"window\": window, \"tenor\": int(tenor), \"spec\": spec,\n",
    "                    \"estimate\": est, \"se\": se,\n",
    "                    \"ci_low\": est - 1.96 * se if pd.notna(est) and pd.notna(se) else np.nan,\n",
    "                    \"ci_high\": est + 1.96 * se if pd.notna(est) and pd.notna(se) else np.nan,\n",
    "                    \"N\": n,\n",
    "                })\n",
    "jump_results = pd.DataFrame(jump_rows)\n",
    "jump_results.to_csv(run_dir / \"tables\" / \"jump_results.csv\", index=False)\n",
    "jump_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bin_rows = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for tenor, g in panel_long.groupby(\"tenor\"):\n",
    "        for spec, controls_set in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "            es = event_study_regression(g, y_col=\"y\", event_date=event, bins=CONFIG[\"event_bins\"], controls=controls_set, hac_lags=CONFIG[\"hac_lags\"])\n",
    "            if es.empty:\n",
    "                continue\n",
    "            es[\"event\"] = event\n",
    "            es[\"tenor\"] = int(tenor)\n",
    "            es[\"spec\"] = spec\n",
    "            bin_rows.append(es)\n",
    "\n",
    "            plot_df = es.sort_values(\"term\")\n",
    "            fig, ax = plt.subplots(figsize=(8, 4))\n",
    "            ax.plot(plot_df[\"term\"], plot_df[\"estimate\"], marker=\"o\")\n",
    "            ax.fill_between(plot_df[\"term\"], plot_df[\"ci_low\"], plot_df[\"ci_high\"], alpha=0.2)\n",
    "            ax.axhline(0, color=\"black\", linewidth=1)\n",
    "            ax.tick_params(axis=\"x\", rotation=45)\n",
    "            ax.set_title(f\"Event={event} tenor={int(tenor)} spec={spec}\")\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(run_dir / \"figures\" / f\"event_path_arb_{int(tenor)}y_{event}_{spec.lower()}.png\", dpi=150)\n",
    "            plt.close(fig)\n",
    "\n",
    "eventstudy_bins = pd.concat(bin_rows, ignore_index=True) if bin_rows else pd.DataFrame()\n",
    "eventstudy_bins.to_csv(run_dir / \"tables\" / \"eventstudy_bins.csv\", index=False)\n",
    "eventstudy_bins.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_rows = []\n",
    "models = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    sub = add_event_time(panel_long, event)\n",
    "    sub = sub[sub[\"event_time\"].between(-60, 60)].copy()\n",
    "    sub[\"post\"] = (sub[\"event_time\"] >= 0).astype(int)\n",
    "    for spec, controls_set in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "        use_controls = [c for c in controls_set if c in sub.columns]\n",
    "        use_cols = [\"y\", \"post\", \"tenor\", *use_controls]\n",
    "        reg = sub[use_cols].dropna().copy()\n",
    "        if reg.empty:\n",
    "            continue\n",
    "        rhs = \"post + C(tenor)\"\n",
    "        if use_controls:\n",
    "            rhs += \" + \" + \" + \".join(use_controls)\n",
    "        res = ols(f\"y ~ {rhs}\", data=reg).fit()\n",
    "        robust = res.get_robustcov_results(cov_type=\"HAC\", maxlags=CONFIG[\"hac_lags\"])\n",
    "        models.append(robust)\n",
    "        if \"post\" in robust.model.exog_names:\n",
    "            i = robust.model.exog_names.index(\"post\")\n",
    "            pooled_rows.append({\"event\": event, \"spec\": spec, \"post\": robust.params[i], \"se\": robust.bse[i], \"N\": int(robust.nobs)})\n",
    "\n",
    "pd.DataFrame(pooled_rows).to_csv(run_dir / \"tables\" / \"pooled_jump_results.csv\", index=False)\n",
    "\n",
    "html_path = run_dir / \"tables\" / \"regression_table.html\"\n",
    "try:\n",
    "    from stargazer.stargazer import Stargazer\n",
    "    if models:\n",
    "        sg = Stargazer(models)\n",
    "        sg.title(\"Pooled jump regressions (HAC SE)\")\n",
    "        html_path.write_text(sg.render_html(), encoding=\"utf-8\")\n",
    "    else:\n",
    "        html_path.write_text(\"<html><body><p>No pooled models available.</p></body></html>\", encoding=\"utf-8\")\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    html_path.write_text(f\"<html><body><p>Stargazer unavailable: {exc}</p></body></html>\", encoding=\"utf-8\")\n",
    "\n",
    "pd.DataFrame(pooled_rows).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_note = \"\"\n",
    "try:\n",
    "    pd_long = load_any_table(resolve_dataset_path(\"primary_dealer_stats_ofr_stfm_nypd_long\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    bank = load_any_table(resolve_dataset_path(\"bank_exposure_y9c_agg_daily\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "\n",
    "    pd_long[\"date\"] = pd.to_datetime(pd_long[\"date\"], errors=\"coerce\")\n",
    "    bank[\"date\"] = pd.to_datetime(bank[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    util_w = pd_long.pivot_table(index=\"date\", columns=\"mnemonic\", values=\"value\", aggfunc=\"mean\").resample(\"W-FRI\").mean()\n",
    "    util_w[\"utilization_index\"] = util_w.sum(axis=1, min_count=1)\n",
    "    util_w[\"utilization_lag1w\"] = util_w[\"utilization_index\"].shift(1)\n",
    "\n",
    "    bank_w = bank.set_index(\"date\").resample(\"W-FRI\").mean()[[\"agg_exempt_share\"]]\n",
    "    y_w = panel_long.groupby([pd.Grouper(key=\"date\", freq=\"W-FRI\")])[\"y\"].mean().to_frame(\"y\")\n",
    "    c_w = panel_long.set_index(\"date\")[[c for c in CONFIG[\"direct_controls\"] if c in panel_long.columns]].resample(\"W-FRI\").mean()\n",
    "\n",
    "    mech = y_w.join([bank_w, util_w[[\"utilization_lag1w\"]], c_w], how=\"inner\").dropna()\n",
    "    mech[\"relief\"] = ((mech.index >= \"2020-04-01\") & (mech.index <= \"2021-03-31\")).astype(int)\n",
    "    mech[\"z_exempt\"] = (mech[\"agg_exempt_share\"] - mech[\"agg_exempt_share\"].mean()) / mech[\"agg_exempt_share\"].std()\n",
    "    mech[\"z_util_l1\"] = (mech[\"utilization_lag1w\"] - mech[\"utilization_lag1w\"].mean()) / mech[\"utilization_lag1w\"].std()\n",
    "    mech[\"relief_x_exempt\"] = mech[\"relief\"] * mech[\"z_exempt\"]\n",
    "    mech[\"relief_x_util\"] = mech[\"relief\"] * mech[\"z_util_l1\"]\n",
    "\n",
    "    xcols = [\"relief\", \"relief_x_exempt\", \"relief_x_util\", *[c for c in CONFIG[\"direct_controls\"] if c in mech.columns]]\n",
    "    reg = mech[[\"y\", *xcols]].dropna()\n",
    "    X = sm.add_constant(reg[xcols], has_constant=\"add\")\n",
    "    res = sm.OLS(reg[\"y\"], X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 2})\n",
    "    pd.DataFrame({\"term\": res.params.index, \"coef\": res.params.values, \"se\": res.bse.values}).to_csv(run_dir / \"tables\" / \"layer2_mechanism_weekly.csv\", index=False)\n",
    "    layer2_note = \"Layer 2 executed.\"\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    layer2_note = f\"Layer 2 skipped gracefully: {exc}\"\n",
    "\n",
    "layer2_note\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_dir.exists():\n",
    "    shutil.rmtree(latest_dir)\n",
    "shutil.copytree(run_dir, latest_dir)\n",
    "\n",
    "notes = {\n",
    "    \"run_dir\": str(run_dir),\n",
    "    \"controls_source\": controls_source,\n",
    "    \"outcomes_source\": str(outcome_path),\n",
    "    \"arb_columns\": arb_cols,\n",
    "    \"unit_note\": unit_note,\n",
    "    \"layer2_note\": layer2_note,\n",
    "}\n",
    "(run_dir / \"README.md\").write_text(\"# Summary pipeline run\\n\\n```json\\n\" + json.dumps(notes, indent=2) + \"\\n```\\n\", encoding=\"utf-8\")\n",
    "notes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}