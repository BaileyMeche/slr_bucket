{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed80a81",
   "metadata": {},
   "source": [
    "# v8_summary_pipeline\n",
    "\n",
    "Self-contained notebook to run the **two-layer econometric design**:\n",
    "\n",
    "- **Layer 1 (Identification)**: daily event-window event studies (TOTAL vs DIRECT) with Newey–West HAC, plus jump estimates.\n",
    "- **Layer 2 (Mechanism)**: weekly regressions of spread changes on `Relief`, `Relief×z(bank_exposure)`, `Relief×z(dealer_util_lag1w)` with HAC.\n",
    "\n",
    "This notebook does **not** import any repo-specific modules.\n",
    "\n",
    "## Expected inputs (edit in CELL 1)\n",
    "You can point the notebook at **any** daily spread dataset. Minimum requirement:\n",
    "\n",
    "- a `date` column, and\n",
    "- one of:\n",
    "  - **wide**: multiple spread columns (e.g., `arb_2`, `arb_5`, ...)\n",
    "  - **long**: `date`, `series_id`, `value`\n",
    "\n",
    "Optional daily controls can be:\n",
    "- in the same file, or\n",
    "- provided as separate files (repo rates, VIX/credit, issuance, etc.)\n",
    "\n",
    "Optional mechanism inputs:\n",
    "- `bank_exposure_y9c_agg_daily.csv` (or any daily bank exposure proxy)\n",
    "- `primary_dealer_stats_ofr_stfm_nypd_long.csv` (weekly; used to build dealer utilization index)\n",
    "\n",
    "## Outputs (written when you run the notebook)\n",
    "All outputs go to:\n",
    "\n",
    "- `../data/v8_summary/`\n",
    "  - `layer1_eventstudy_coeffs.csv`\n",
    "  - `layer1_jump_estimates.csv`\n",
    "  - `layer2_weekly_panel.csv`\n",
    "  - `layer2_mechanism_results.csv`\n",
    "- `../data/v8_summary/figures/`\n",
    "  - time-series plots\n",
    "  - event-study coefficient plots\n",
    "  - mechanism coefficient plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7321a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: C:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\outputs\\v8_summary\n",
      "FIG_DIR: C:\\Users\\Owner\\Box\\Winter26\\slr_bucket\\outputs\\v8_summary\\figures\n"
     ]
    }
   ],
   "source": [
    "# CELL 1 — Configuration (edit paths here)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Core user inputs\n",
    "# -----------------------\n",
    "# Daily spreads / wedge file (CSV or Parquet). Must include 'date' and either:\n",
    "#  - long:  date, series_id, value\n",
    "#  - wide:  date + multiple spread columns (you will select columns by patterns below)\n",
    "DAILY_SPREADS_PATH = None  # e.g., Path(\"_data/tips_treasury_implied_rf_2010.parquet\") or Path(\"event_study_start_v4_wide.csv\")\n",
    "\n",
    "# Optional: daily controls (if not inside DAILY_SPREADS_PATH)\n",
    "REPO_RATES_PATH   = Path(\"../data/event_inputs/repo_rates_combined.csv\")\n",
    "RISK_CONTROLS_PATH = Path(\"../data/event_inputs/controls_vix_creditspreads_fred.csv\")\n",
    "ISSUANCE_PATH     = Path(\"../data/event_inputs/treasury_issuance_by_tenor_fiscaldata.csv\")\n",
    "\n",
    "# Optional mechanism inputs\n",
    "BANK_EXPOSURE_DAILY_PATH = Path(\"../data/event_inputs/bank_exposure_y9c_agg_daily.csv\")  # must have date + some exposure share column\n",
    "PRIMARY_DEALER_LONG_PATH = Path(\"../data/event_inputs/primary_dealer_stats_ofr_stfm_nypd_long.csv\")  # must have date,mnemonic,value\n",
    "\n",
    "# If your files are somewhere else, set these paths explicitly.\n",
    "# Example:\n",
    "# DAILY_SPREADS_PATH = Path(\"/path/to/event_study_start_v4_wide.csv\")\n",
    "\n",
    "# -----------------------\n",
    "# Column inference settings (wide daily file)\n",
    "# -----------------------\n",
    "# Patterns used to pick \"spread\" columns if your daily file is wide.\n",
    "# Edit these if your names differ.\n",
    "SPREAD_COL_PATTERNS = [\n",
    "    r\"\\barb[_\\-]?\\d+\\b\",                 # arb_2, arb_5, ...\n",
    "    r\"\\bwedge(_bps)?[_\\-]?\\d+\\b\",        # wedge_2, wedge_bps_2, ...\n",
    "    r\"\\btips_treas[_\\-]?\\d+\\b\",          # tips_treas_2_rf, ...\n",
    "]\n",
    "\n",
    "# Controls (DIRECT spec) columns to use if available in merged daily panel\n",
    "DIRECT_CONTROL_CANDS = [\n",
    "    \"SOFR\",\n",
    "    \"spr_tgcr\",        # TGCR–SOFR spread (preferred)\n",
    "    \"spr_effr\",        # SOFR–EFFR or EFFR–SOFR depending on your construction\n",
    "    \"TGCR_minus_SOFR\", # derived if TGCR and SOFR exist\n",
    "    \"BGCR_minus_SOFR\", # derived if BGCR and SOFR exist\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# Event dates and windows\n",
    "# -----------------------\n",
    "EVENTS = [\n",
    "    (\"start_effective\", pd.Timestamp(\"2020-04-01\")),\n",
    "    (\"exit_announce\",   pd.Timestamp(\"2021-03-19\")),\n",
    "    (\"expiry\",          pd.Timestamp(\"2021-03-31\")),\n",
    "]\n",
    "WINDOWS = [20, 60]          # event window half-width W in days\n",
    "OMIT_K  = -1                # omitted baseline day\n",
    "NW_LAGS_DAILY = 10          # HAC lags for daily\n",
    "NW_LAGS_WEEKLY = 4          # HAC lags for weekly\n",
    "\n",
    "# Relief indicator window for mechanism layer\n",
    "RELIEF_START = pd.Timestamp(\"2020-04-01\")\n",
    "RELIEF_END   = pd.Timestamp(\"2021-03-31\")\n",
    "\n",
    "# -----------------------\n",
    "# Output directory\n",
    "# -----------------------\n",
    "OUT_DIR = Path(\"../outputs/v8_summary\")\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"OUT_DIR:\", OUT_DIR.resolve())\n",
    "print(\"FIG_DIR:\", FIG_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22387047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 — Imports + helper functions (self-contained)\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def _ensure_date(df, col=\"date\"):\n",
    "    if col not in df.columns:\n",
    "        # attempt common variants\n",
    "        for alt in [\"Date\", \"DATE\", \"dt\", \"tradeDate\", \"tradedate\"]:\n",
    "            if alt in df.columns:\n",
    "                df = df.rename(columns={alt: col})\n",
    "                break\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing '{col}' column. cols={df.columns.tolist()}\")\n",
    "    out = df.copy()\n",
    "    out[col] = pd.to_datetime(out[col])\n",
    "    return out\n",
    "\n",
    "def _read_any(path: Path):\n",
    "    if path is None:\n",
    "        return None\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    if path.suffix.lower() == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def _pick_cols_by_patterns(df, patterns):\n",
    "    cols = df.columns.tolist()\n",
    "    keep = set()\n",
    "    for pat in patterns:\n",
    "        rx = re.compile(pat, re.IGNORECASE)\n",
    "        for c in cols:\n",
    "            if rx.search(c):\n",
    "                keep.add(c)\n",
    "    return sorted(keep)\n",
    "\n",
    "def _wide_to_long_spreads(df_wide, date_col=\"date\", value_name=\"value\"):\n",
    "    # picks spread columns by patterns and melts\n",
    "    spread_cols = _pick_cols_by_patterns(df_wide, SPREAD_COL_PATTERNS)\n",
    "    if not spread_cols:\n",
    "        raise ValueError(\n",
    "            \"Could not infer spread columns from wide daily file. \"\n",
    "            f\"Try editing SPREAD_COL_PATTERNS. cols={df_wide.columns.tolist()}\"\n",
    "        )\n",
    "    out = df_wide[[date_col] + spread_cols].melt(id_vars=[date_col], var_name=\"series_id\", value_name=value_name)\n",
    "    return out, spread_cols\n",
    "\n",
    "def _standardize_z(x):\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return (x - x.mean()) / x.std(ddof=0)\n",
    "\n",
    "def _event_dummies(dates: pd.Series, t0: pd.Timestamp, W: int, omit_k: int):\n",
    "    k = (dates - t0).dt.days.astype(int)\n",
    "    d = {}\n",
    "    for kk in range(-W, W+1):\n",
    "        if kk == omit_k:\n",
    "            continue\n",
    "        d[f\"k_{kk:+d}\"] = (k == kk).astype(int)\n",
    "    D = pd.DataFrame(d)\n",
    "    return D, k\n",
    "\n",
    "def _nw_ols(y, X, lags: int):\n",
    "    \"\"\"\n",
    "    OLS with Newey–West HAC, robust to object dtypes.\n",
    "    Forces numeric, drops all-NaN and zero-variance columns, aligns y/X.\n",
    "    \"\"\"\n",
    "    # y -> numeric\n",
    "    y = pd.to_numeric(pd.Series(y), errors=\"coerce\")\n",
    "\n",
    "    # X -> numeric (coerce objects/bools)\n",
    "    Xc = sm.add_constant(X, has_constant=\"add\").copy()\n",
    "\n",
    "    # Convert booleans explicitly (sometimes they stay object after concat)\n",
    "    for c in Xc.columns:\n",
    "        if Xc[c].dtype == bool:\n",
    "            Xc[c] = Xc[c].astype(float)\n",
    "\n",
    "    # Coerce everything else to numeric\n",
    "    Xc = Xc.apply(lambda s: pd.to_numeric(s, errors=\"coerce\"))\n",
    "\n",
    "    # Drop columns that are entirely missing\n",
    "    Xc = Xc.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Drop columns with zero variance (can appear if a dummy is all zeros in window)\n",
    "    nunique = Xc.nunique(dropna=True)\n",
    "    Xc = Xc.loc[:, nunique > 1]\n",
    "\n",
    "    # Align and drop rows with any missing in y or X\n",
    "    df = pd.concat([y.rename(\"y\"), Xc], axis=1)\n",
    "    df = df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "    y2 = df[\"y\"].astype(float).values\n",
    "    X2 = df.drop(columns=[\"y\"]).astype(float).values\n",
    "\n",
    "    res = sm.OLS(y2, X2).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": lags})\n",
    "    # Attach param names back for convenience\n",
    "    res.model.data.xnames = list(df.drop(columns=[\"y\"]).columns)\n",
    "    return res\n",
    "\n",
    "def _run_eventstudy_one(df, ycol, t0, W, controls=None, omit_k=-1, lags=10):\n",
    "    D, k = _event_dummies(df[\"date\"], t0, W, omit_k)\n",
    "    X = D.copy()\n",
    "    if controls:\n",
    "        for c in controls:\n",
    "            if c in df.columns:\n",
    "                X[c] = df[c]\n",
    "    res = _nw_ols(df[ycol], X, lags=lags)\n",
    "    out = []\n",
    "    for name in D.columns:\n",
    "        kk = int(name.replace(\"k_\", \"\").replace(\"+\",\"\"))\n",
    "        beta = res.params.get(name, np.nan)\n",
    "        se   = res.bse.get(name, np.nan)\n",
    "        out.append({\"k\": kk, \"beta\": beta, \"se\": se})\n",
    "    out = pd.DataFrame(out).sort_values(\"k\")\n",
    "    out[\"lo\"] = out[\"beta\"] - 1.96*out[\"se\"]\n",
    "    out[\"hi\"] = out[\"beta\"] + 1.96*out[\"se\"]\n",
    "    return out\n",
    "\n",
    "def _run_eventstudy_pooled(long_df, t0, W, controls=None, omit_k=-1, lags=10):\n",
    "    # long_df: date, series_id, value plus controls\n",
    "    D, k = _event_dummies(long_df[\"date\"], t0, W, omit_k)\n",
    "    X = D.copy()\n",
    "    # series FE via one-hot (drop first)\n",
    "    fe = pd.get_dummies(long_df[\"series_id\"], prefix=\"fe\", drop_first=True)\n",
    "    X = pd.concat([X.reset_index(drop=True), fe.reset_index(drop=True)], axis=1)\n",
    "    if controls:\n",
    "        for c in controls:\n",
    "            if c in long_df.columns:\n",
    "                X[c] = long_df[c].values\n",
    "\n",
    "    # ensure no stray non-numeric columns\n",
    "    X = X.copy()\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == bool:\n",
    "            X[c] = X[c].astype(float)\n",
    "\n",
    "    res = _nw_ols(long_df[\"value\"], X, lags=lags)\n",
    "    out = []\n",
    "    for col in D.columns:\n",
    "        kk = int(col.replace(\"k_\", \"\").replace(\"+\",\"\"))\n",
    "        out.append({\"k\": kk, \"beta\": res.params.get(col, np.nan), \"se\": res.bse.get(col, np.nan)})\n",
    "    out = pd.DataFrame(out).sort_values(\"k\")\n",
    "    out[\"lo\"] = out[\"beta\"] - 1.96*out[\"se\"]\n",
    "    out[\"hi\"] = out[\"beta\"] + 1.96*out[\"se\"]\n",
    "    return out\n",
    "\n",
    "def _jump_estimate(df, ycol, t0, W):\n",
    "    # mean post (k in [0,W]) minus mean pre (k in [-W,-1])\n",
    "    k = (df[\"date\"] - t0).dt.days.astype(int)\n",
    "    pre = df.loc[(k >= -W) & (k <= -1), ycol]\n",
    "    post = df.loc[(k >= 0) & (k <= W), ycol]\n",
    "    return float(post.mean() - pre.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f252901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily spreads file: ..\\data\\tips_treasury_implied_rf_2010.parquet\n",
      "Series count: 4\n",
      "Example series: ['arb_10', 'arb_2', 'arb_20', 'arb_5']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>series_id</th>\n",
       "      <th>value</th>\n",
       "      <th>real_cc2</th>\n",
       "      <th>real_cc5</th>\n",
       "      <th>real_cc10</th>\n",
       "      <th>real_cc20</th>\n",
       "      <th>nom_zc2</th>\n",
       "      <th>nom_zc5</th>\n",
       "      <th>nom_zc10</th>\n",
       "      <th>...</th>\n",
       "      <th>SOFR</th>\n",
       "      <th>TGCR</th>\n",
       "      <th>BGCR</th>\n",
       "      <th>sofr_minus_tgcr</th>\n",
       "      <th>sofr_minus_bgcr</th>\n",
       "      <th>VIX</th>\n",
       "      <th>HY_OAS</th>\n",
       "      <th>BAA10Y</th>\n",
       "      <th>TGCR_minus_SOFR</th>\n",
       "      <th>BGCR_minus_SOFR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>arb_10</td>\n",
       "      <td>23.935082</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>0.016359</td>\n",
       "      <td>0.021538</td>\n",
       "      <td>109.505229</td>\n",
       "      <td>268.439780</td>\n",
       "      <td>428.047942</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>arb_10</td>\n",
       "      <td>22.439776</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.015905</td>\n",
       "      <td>0.020813</td>\n",
       "      <td>101.824926</td>\n",
       "      <td>259.540905</td>\n",
       "      <td>420.521627</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>arb_10</td>\n",
       "      <td>19.821813</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>0.006443</td>\n",
       "      <td>0.016328</td>\n",
       "      <td>0.021353</td>\n",
       "      <td>102.875564</td>\n",
       "      <td>264.569315</td>\n",
       "      <td>428.767485</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>arb_10</td>\n",
       "      <td>21.670143</td>\n",
       "      <td>-0.002250</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>0.015919</td>\n",
       "      <td>0.021503</td>\n",
       "      <td>104.805405</td>\n",
       "      <td>266.283644</td>\n",
       "      <td>427.808072</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>arb_10</td>\n",
       "      <td>22.848053</td>\n",
       "      <td>-0.002142</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>98.996805</td>\n",
       "      <td>262.434519</td>\n",
       "      <td>426.160609</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date series_id      value  real_cc2  real_cc5  real_cc10  real_cc20  \\\n",
       "0 2010-01-04    arb_10  23.935082 -0.000515  0.007010   0.016359   0.021538   \n",
       "1 2010-01-05    arb_10  22.439776 -0.001248  0.006286   0.015905   0.020813   \n",
       "2 2010-01-06    arb_10  19.821813 -0.001413  0.006443   0.016328   0.021353   \n",
       "3 2010-01-07    arb_10  21.670143 -0.002250  0.006208   0.015919   0.021503   \n",
       "4 2010-01-08    arb_10  22.848053 -0.002142  0.005842   0.015771   0.021135   \n",
       "\n",
       "      nom_zc2     nom_zc5    nom_zc10  ...  SOFR  TGCR  BGCR  sofr_minus_tgcr  \\\n",
       "0  109.505229  268.439780  428.047942  ...   NaN   NaN   NaN              NaN   \n",
       "1  101.824926  259.540905  420.521627  ...   NaN   NaN   NaN              NaN   \n",
       "2  102.875564  264.569315  428.767485  ...   NaN   NaN   NaN              NaN   \n",
       "3  104.805405  266.283644  427.808072  ...   NaN   NaN   NaN              NaN   \n",
       "4   98.996805  262.434519  426.160609  ...   NaN   NaN   NaN              NaN   \n",
       "\n",
       "   sofr_minus_bgcr  VIX  HY_OAS  BAA10Y  TGCR_minus_SOFR  BGCR_minus_SOFR  \n",
       "0              NaN  NaN     NaN     NaN              NaN              NaN  \n",
       "1              NaN  NaN     NaN     NaN              NaN              NaN  \n",
       "2              NaN  NaN     NaN     NaN              NaN              NaN  \n",
       "3              NaN  NaN     NaN     NaN              NaN              NaN  \n",
       "4              NaN  NaN     NaN     NaN              NaN              NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 3 — Load inputs and construct the daily analysis panel\n",
    "\n",
    "# 1) Load daily spreads\n",
    "if DAILY_SPREADS_PATH is None:\n",
    "    # try a few common locations if not provided\n",
    "    DAILY_SPREADS_PATH = next((p for p in [\n",
    "        Path(\"../data/tips_treasury_implied_rf_2010.parquet\"),\n",
    "    ] if p.exists()), None)\n",
    "\n",
    "if DAILY_SPREADS_PATH is None:\n",
    "    raise FileNotFoundError(\"Set DAILY_SPREADS_PATH to your daily spreads file.\")\n",
    "\n",
    "daily_raw = _read_any(DAILY_SPREADS_PATH)\n",
    "daily_raw = _ensure_date(daily_raw, \"date\").sort_values(\"date\")\n",
    "\n",
    "# 2) Convert to long spreads (date, series_id, value)\n",
    "if {\"series_id\",\"value\"}.issubset(daily_raw.columns):\n",
    "    daily_long = daily_raw[[\"date\",\"series_id\",\"value\"]].copy()\n",
    "    spread_cols = sorted(daily_long[\"series_id\"].unique())\n",
    "else:\n",
    "    # wide -> long\n",
    "    daily_long, spread_cols = _wide_to_long_spreads(daily_raw, \"date\", \"value\")\n",
    "\n",
    "# 3) Merge daily controls (if available)\n",
    "repo = _read_any(REPO_RATES_PATH)\n",
    "risk = _read_any(RISK_CONTROLS_PATH)\n",
    "\n",
    "if repo is not None:\n",
    "    repo = _ensure_date(repo, \"date\").sort_values(\"date\")\n",
    "if risk is not None:\n",
    "    risk = _ensure_date(risk, \"date\").sort_values(\"date\")\n",
    "\n",
    "# merge controls into wide daily panel (for per-series regressions)\n",
    "daily_wide = daily_raw.copy()\n",
    "if repo is not None:\n",
    "    daily_wide = daily_wide.merge(repo, on=\"date\", how=\"left\")\n",
    "if risk is not None:\n",
    "    daily_wide = daily_wide.merge(risk, on=\"date\", how=\"left\")\n",
    "\n",
    "# derive TGCR/BGCR spreads if present\n",
    "if \"SOFR\" in daily_wide.columns and \"TGCR\" in daily_wide.columns and \"TGCR_minus_SOFR\" not in daily_wide.columns:\n",
    "    daily_wide[\"TGCR_minus_SOFR\"] = daily_wide[\"TGCR\"] - daily_wide[\"SOFR\"]\n",
    "if \"SOFR\" in daily_wide.columns and \"BGCR\" in daily_wide.columns and \"BGCR_minus_SOFR\" not in daily_wide.columns:\n",
    "    daily_wide[\"BGCR_minus_SOFR\"] = daily_wide[\"BGCR\"] - daily_wide[\"SOFR\"]\n",
    "\n",
    "# also merge controls into long panel for pooled regressions\n",
    "daily_long = daily_long.merge(daily_wide.drop(columns=spread_cols, errors=\"ignore\"), on=\"date\", how=\"left\")\n",
    "\n",
    "print(\"Daily spreads file:\", DAILY_SPREADS_PATH)\n",
    "print(\"Series count:\", len(spread_cols))\n",
    "print(\"Example series:\", spread_cols[:10])\n",
    "daily_long.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbaca24f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ev_name, t0 \u001b[38;5;129;01min\u001b[39;00m EVENTS:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m W \u001b[38;5;129;01min\u001b[39;00m WINDOWS:\n\u001b[32m     13\u001b[39m         \u001b[38;5;66;03m# pooled event study (series FE) — TOTAL\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         pooled_total = \u001b[43m_run_eventstudy_pooled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily_long\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momit_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOMIT_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNW_LAGS_DAILY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m         pooled_total[\u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m] = ev_name\n\u001b[32m     16\u001b[39m         pooled_total[\u001b[33m\"\u001b[39m\u001b[33mt0\u001b[39m\u001b[33m\"\u001b[39m] = t0\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36m_run_eventstudy_pooled\u001b[39m\u001b[34m(long_df, t0, W, controls, omit_k, lags)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m D.columns:\n\u001b[32m    149\u001b[39m     kk = \u001b[38;5;28mint\u001b[39m(col.replace(\u001b[33m\"\u001b[39m\u001b[33mk_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m+\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     out.append({\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: kk, \u001b[33m\"\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(col, np.nan), \u001b[33m\"\u001b[39m\u001b[33mse\u001b[39m\u001b[33m\"\u001b[39m: res.bse.get(col, np.nan)})\n\u001b[32m    151\u001b[39m out = pd.DataFrame(out).sort_values(\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    152\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mlo\u001b[39m\u001b[33m\"\u001b[39m] = out[\u001b[33m\"\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m\"\u001b[39m] - \u001b[32m1.96\u001b[39m*out[\u001b[33m\"\u001b[39m\u001b[33mse\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# CELL 4 — Layer 1: Event-study regressions (TOTAL vs DIRECT) + jump estimates\n",
    "# Outputs:\n",
    "#  - layer1_eventstudy_coeffs.csv\n",
    "#  - layer1_jump_estimates.csv\n",
    "\n",
    "direct_controls = [c for c in DIRECT_CONTROL_CANDS if c in daily_long.columns]\n",
    "\n",
    "coeff_rows = []\n",
    "jump_rows = []\n",
    "\n",
    "for ev_name, t0 in EVENTS:\n",
    "    for W in WINDOWS:\n",
    "        # pooled event study (series FE) — TOTAL\n",
    "        pooled_total = _run_eventstudy_pooled(daily_long, t0, W, controls=[], omit_k=OMIT_K, lags=NW_LAGS_DAILY)\n",
    "        pooled_total[\"event\"] = ev_name\n",
    "        pooled_total[\"t0\"] = t0\n",
    "        pooled_total[\"W\"] = W\n",
    "        pooled_total[\"spec\"] = \"TOTAL\"\n",
    "        pooled_total[\"series_id\"] = \"__pooled__\"\n",
    "        coeff_rows.append(pooled_total)\n",
    "\n",
    "        # pooled — DIRECT (controls)\n",
    "        pooled_direct = _run_eventstudy_pooled(daily_long, t0, W, controls=direct_controls, omit_k=OMIT_K, lags=NW_LAGS_DAILY)\n",
    "        pooled_direct[\"event\"] = ev_name\n",
    "        pooled_direct[\"t0\"] = t0\n",
    "        pooled_direct[\"W\"] = W\n",
    "        pooled_direct[\"spec\"] = \"DIRECT\"\n",
    "        pooled_direct[\"series_id\"] = \"__pooled__\"\n",
    "        coeff_rows.append(pooled_direct)\n",
    "\n",
    "        # separate regressions by series\n",
    "        for sid in spread_cols:\n",
    "            sub = daily_wide[[\"date\"]].copy()\n",
    "            # attach outcome from original raw file if wide; otherwise pull from long\n",
    "            if sid in daily_wide.columns:\n",
    "                sub[\"y\"] = pd.to_numeric(daily_wide[sid], errors=\"coerce\")\n",
    "            else:\n",
    "                sub = sub.merge(daily_long[daily_long[\"series_id\"]==sid][[\"date\",\"value\"]], on=\"date\", how=\"left\")\n",
    "                sub[\"y\"] = pd.to_numeric(sub[\"value\"], errors=\"coerce\")\n",
    "\n",
    "            # attach controls\n",
    "            for c in direct_controls:\n",
    "                sub[c] = daily_wide[c].values if c in daily_wide.columns else np.nan\n",
    "\n",
    "            sub = sub.dropna(subset=[\"y\"]).sort_values(\"date\")\n",
    "\n",
    "            # TOTAL\n",
    "            est_tot = _run_eventstudy_one(sub.rename(columns={\"y\":\"y\"}), \"y\", t0, W, controls=[], omit_k=OMIT_K, lags=NW_LAGS_DAILY)\n",
    "            est_tot[\"event\"] = ev_name\n",
    "            est_tot[\"t0\"] = t0\n",
    "            est_tot[\"W\"] = W\n",
    "            est_tot[\"spec\"] = \"TOTAL\"\n",
    "            est_tot[\"series_id\"] = sid\n",
    "            coeff_rows.append(est_tot)\n",
    "\n",
    "            # DIRECT\n",
    "            est_dir = _run_eventstudy_one(sub.rename(columns={\"y\":\"y\"}), \"y\", t0, W, controls=direct_controls, omit_k=OMIT_K, lags=NW_LAGS_DAILY)\n",
    "            est_dir[\"event\"] = ev_name\n",
    "            est_dir[\"t0\"] = t0\n",
    "            est_dir[\"W\"] = W\n",
    "            est_dir[\"spec\"] = \"DIRECT\"\n",
    "            est_dir[\"series_id\"] = sid\n",
    "            coeff_rows.append(est_dir)\n",
    "\n",
    "            # jump estimate (TOTAL only, using raw y)\n",
    "            jump = _jump_estimate(sub.rename(columns={\"y\":\"y\"}), \"y\", t0, W)\n",
    "            jump_rows.append({\n",
    "                \"event\": ev_name,\n",
    "                \"t0\": t0,\n",
    "                \"W\": W,\n",
    "                \"series_id\": sid,\n",
    "                \"jump_post_minus_pre\": jump\n",
    "            })\n",
    "\n",
    "coeffs = pd.concat(coeff_rows, ignore_index=True)\n",
    "jumps = pd.DataFrame(jump_rows)\n",
    "\n",
    "coeff_path = OUT_DIR / \"layer1_eventstudy_coeffs.csv\"\n",
    "jump_path  = OUT_DIR / \"layer1_jump_estimates.csv\"\n",
    "coeffs.to_csv(coeff_path, index=False)\n",
    "jumps.to_csv(jump_path, index=False)\n",
    "\n",
    "print(\"Saved:\", coeff_path)\n",
    "print(\"Saved:\", jump_path)\n",
    "print(\"DIRECT controls used:\", direct_controls)\n",
    "coeffs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cda565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 — Layer 1 plots: (i) time-series per series + event lines, (ii) event-study coefficient panels\n",
    "# Saves PNGs into FIG_DIR\n",
    "\n",
    "# (i) time-series per series (first 12 series to avoid huge output; edit if needed)\n",
    "series_to_plot = spread_cols[:12]\n",
    "for sid in series_to_plot:\n",
    "    s = daily_wide[[\"date\"]].copy()\n",
    "    if sid in daily_wide.columns:\n",
    "        s[\"y\"] = pd.to_numeric(daily_wide[sid], errors=\"coerce\")\n",
    "    else:\n",
    "        s = s.merge(daily_long[daily_long[\"series_id\"]==sid][[\"date\",\"value\"]], on=\"date\", how=\"left\")\n",
    "        s[\"y\"] = pd.to_numeric(s[\"value\"], errors=\"coerce\")\n",
    "\n",
    "    s = s.dropna(subset=[\"y\"]).sort_values(\"date\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    ax.plot(s[\"date\"], s[\"y\"], label=sid)\n",
    "    for d, _ in EVENTS:\n",
    "        ax.axvline(dict(EVENTS)[d], linestyle=\"--\", linewidth=1)\n",
    "    ax.set_title(f\"Daily series: {sid}\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Units of series (as provided)\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / f\"ts_{re.sub('[^A-Za-z0-9_\\-]+','_',sid)}.png\", dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "# (ii) event-study coefficient panels: per event, per W, pooled and first 4 series\n",
    "plot_series = [\"__pooled__\"] + spread_cols[:4]\n",
    "\n",
    "for ev_name, t0 in EVENTS:\n",
    "    for W in WINDOWS:\n",
    "        sub = coeffs[(coeffs[\"event\"]==ev_name) & (coeffs[\"W\"]==W) & (coeffs[\"series_id\"].isin(plot_series))].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        # grid: rows=series, cols=spec (TOTAL, DIRECT)\n",
    "        series_list = plot_series\n",
    "        fig, axes = plt.subplots(len(series_list), 1, figsize=(12, 3.2*len(series_list)), sharex=True)\n",
    "\n",
    "        if len(series_list)==1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for i, sid in enumerate(series_list):\n",
    "            ax = axes[i]\n",
    "            for spec in [\"TOTAL\",\"DIRECT\"]:\n",
    "                s = sub[(sub[\"series_id\"]==sid) & (sub[\"spec\"]==spec)].sort_values(\"k\")\n",
    "                ax.plot(s[\"k\"], s[\"beta\"], label=spec)\n",
    "                ax.fill_between(s[\"k\"], s[\"lo\"], s[\"hi\"], alpha=0.15)\n",
    "            ax.axvline(0, linestyle=\"--\", linewidth=1)\n",
    "            ax.axhline(0, linestyle=\":\", linewidth=1)\n",
    "            ax.set_title(f\"{ev_name} (t0={t0.date()}) | W={W} | series={sid}\")\n",
    "            ax.set_ylabel(\"beta\")\n",
    "            ax.legend()\n",
    "\n",
    "        axes[-1].set_xlabel(\"event time k (days)\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(FIG_DIR / f\"eventstudy_{ev_name}_W{W}.png\", dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "print(\"Saved figures to:\", FIG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 — Build weekly panel for Layer 2 mechanism regressions\n",
    "# Output: layer2_weekly_panel.csv\n",
    "\n",
    "# Weekly aggregation rule: Wednesday week-ending mean of daily series\n",
    "def to_weekly_mean(long_df):\n",
    "    tmp = long_df.copy()\n",
    "    tmp = tmp.set_index(\"date\")\n",
    "    w = (tmp.groupby(\"series_id\")\n",
    "           .apply(lambda g: g[[\"value\"]].resample(\"W-WED\").mean())\n",
    "           .reset_index()\n",
    "           .rename(columns={\"value\":\"wedge_weekly\"}))\n",
    "    return w\n",
    "\n",
    "# Base: weekly mean of each series\n",
    "base_weekly = daily_long[[\"date\",\"series_id\",\"value\"]].copy()\n",
    "base_weekly[\"value\"] = pd.to_numeric(base_weekly[\"value\"], errors=\"coerce\")\n",
    "base_weekly = base_weekly.dropna(subset=[\"value\"])\n",
    "wk = to_weekly_mean(base_weekly)\n",
    "\n",
    "# Weekly change in spread\n",
    "wk = wk.sort_values([\"series_id\",\"date\"])\n",
    "wk[\"dwedge\"] = wk.groupby(\"series_id\")[\"wedge_weekly\"].diff()\n",
    "\n",
    "# Relief indicator\n",
    "wk[\"Relief\"] = ((wk[\"date\"] >= RELIEF_START) & (wk[\"date\"] <= RELIEF_END)).astype(int)\n",
    "\n",
    "# Bank exposure (daily -> weekly mean -> z-score)\n",
    "bank = _read_any(BANK_EXPOSURE_DAILY_PATH)\n",
    "bank_exposure_col = None\n",
    "if bank is not None:\n",
    "    bank = _ensure_date(bank, \"date\").sort_values(\"date\")\n",
    "    # choose best available exposure column\n",
    "    for c in [\"agg_exempt_share\",\"agg_exempt_share_proxy\",\"agg_reserves_share\",\"agg_exempt_share_min\",\"agg_exempt_share_proxy\"]:\n",
    "        if c in bank.columns:\n",
    "            bank_exposure_col = c\n",
    "            break\n",
    "    if bank_exposure_col is None:\n",
    "        # pick any numeric column other than date\n",
    "        num_cols = [c for c in bank.columns if c != \"date\" and pd.api.types.is_numeric_dtype(bank[c])]\n",
    "        bank_exposure_col = num_cols[0] if num_cols else None\n",
    "\n",
    "if bank is not None and bank_exposure_col is not None:\n",
    "    bank_w = bank.set_index(\"date\")[[bank_exposure_col]].resample(\"W-WED\").mean().reset_index()\n",
    "    bank_w[\"bank_exposure_z\"] = _standardize_z(bank_w[bank_exposure_col])\n",
    "    wk = wk.merge(bank_w[[\"date\",\"bank_exposure_z\"]], on=\"date\", how=\"left\")\n",
    "else:\n",
    "    wk[\"bank_exposure_z\"] = np.nan\n",
    "    print(\"WARNING: bank exposure daily file missing or no usable exposure column; bank_exposure_z will be NaN.\")\n",
    "\n",
    "# Dealer utilization (primary dealer long -> weekly index -> lag1)\n",
    "pd_long = _read_any(PRIMARY_DEALER_LONG_PATH)\n",
    "if pd_long is None:\n",
    "    wk[\"dealer_util_z_lag1w\"] = np.nan\n",
    "    print(\"WARNING: primary dealer file missing; dealer_util_z_lag1w will be NaN.\")\n",
    "else:\n",
    "    pd_long = _ensure_date(pd_long, \"date\").sort_values(\"date\")\n",
    "    if not {\"mnemonic\",\"value\"}.issubset(pd_long.columns):\n",
    "        raise ValueError(f\"Primary dealer file missing mnemonic/value. cols={pd_long.columns.tolist()}\")\n",
    "\n",
    "    # utilization index uses repo financing series if present (fallback to any RP_* Treasury/TIPS series)\n",
    "    rp_cands = [\n",
    "        \"NYPD-PD_RP_T_TOT-A\",\n",
    "        \"NYPD-PD_RP_TIPS_TOT-A\",\n",
    "        \"NYPD-PD_RP_TOT-A\",\n",
    "    ]\n",
    "    available = [c for c in rp_cands if c in set(pd_long[\"mnemonic\"])]\n",
    "    if not available:\n",
    "        available = [m for m in pd_long[\"mnemonic\"].unique() if m.startswith(\"NYPD-PD_RP_\")][:3]\n",
    "\n",
    "    pd_wide = (pd_long[pd_long[\"mnemonic\"].isin(available)]\n",
    "               .pivot_table(index=\"date\", columns=\"mnemonic\", values=\"value\", aggfunc=\"first\")\n",
    "               .sort_index()\n",
    "               .reset_index())\n",
    "\n",
    "    # utilization index = mean z-score across chosen repo series\n",
    "    for c in available:\n",
    "        pd_wide[c] = pd.to_numeric(pd_wide[c], errors=\"coerce\")\n",
    "        pd_wide[f\"z_{c}\"] = _standardize_z(pd_wide[c])\n",
    "    zcols = [f\"z_{c}\" for c in available]\n",
    "    pd_wide[\"dealer_util_z\"] = pd_wide[zcols].mean(axis=1)\n",
    "\n",
    "    # merge and lag\n",
    "    wk = wk.merge(pd_wide[[\"date\",\"dealer_util_z\"]], on=\"date\", how=\"left\")\n",
    "    wk = wk.sort_values([\"series_id\",\"date\"])\n",
    "    wk[\"dealer_util_z_lag1w\"] = wk.groupby(\"series_id\")[\"dealer_util_z\"].shift(1)\n",
    "\n",
    "# Controls (weekly mean from daily controls if present in daily_wide)\n",
    "ctrl_cols = [c for c in [\"VIX\",\"HY_OAS\",\"BAA10Y\",\"SOFR\",\"spr_tgcr\",\"spr_effr\"] if c in daily_wide.columns]\n",
    "if ctrl_cols:\n",
    "    ctrl_w = daily_wide.set_index(\"date\")[ctrl_cols].resample(\"W-WED\").mean().reset_index()\n",
    "    wk = wk.merge(ctrl_w, on=\"date\", how=\"left\")\n",
    "\n",
    "wk_path = OUT_DIR / \"layer2_weekly_panel.csv\"\n",
    "wk.to_csv(wk_path, index=False)\n",
    "print(\"Saved:\", wk_path)\n",
    "wk.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 — Layer 2 mechanism regressions (weekly; by series and pooled)\n",
    "# Model: dwedge = b0 + b1*Relief + b2*(Relief*bank_exposure_z) + b3*(Relief*dealer_util_z_lag1w) + controls + e\n",
    "# HAC: NW_LAGS_WEEKLY\n",
    "# Output: layer2_mechanism_results.csv\n",
    "\n",
    "def run_mech(df, y=\"dwedge\", add_controls=True, lags=4):\n",
    "    X = pd.DataFrame({\n",
    "        \"Relief\": df[\"Relief\"].astype(float),\n",
    "        \"Relief_x_bank\": df[\"Relief\"].astype(float) * df[\"bank_exposure_z\"].astype(float),\n",
    "        \"Relief_x_util\": df[\"Relief\"].astype(float) * df[\"dealer_util_z_lag1w\"].astype(float),\n",
    "    })\n",
    "    if add_controls:\n",
    "        for c in [\"VIX\",\"HY_OAS\",\"BAA10Y\",\"SOFR\",\"spr_tgcr\",\"spr_effr\"]:\n",
    "            if c in df.columns:\n",
    "                X[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    res = _nw_ols(pd.to_numeric(df[y], errors=\"coerce\"), X, lags=lags)\n",
    "    rows = []\n",
    "    for nm in [\"Relief\",\"Relief_x_bank\",\"Relief_x_util\"]:\n",
    "        rows.append({\n",
    "            \"coef\": nm,\n",
    "            \"beta\": res.params.get(nm, np.nan),\n",
    "            \"se\": res.bse.get(nm, np.nan),\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    out[\"lo\"] = out[\"beta\"] - 1.96*out[\"se\"]\n",
    "    out[\"hi\"] = out[\"beta\"] + 1.96*out[\"se\"]\n",
    "    return out\n",
    "\n",
    "results = []\n",
    "\n",
    "# pooled across series (series FE)\n",
    "wk2 = wk.dropna(subset=[\"dwedge\"]).copy()\n",
    "fe = pd.get_dummies(wk2[\"series_id\"], prefix=\"fe\", drop_first=True)\n",
    "X = pd.DataFrame({\n",
    "    \"Relief\": wk2[\"Relief\"].astype(float),\n",
    "    \"Relief_x_bank\": wk2[\"Relief\"].astype(float) * wk2[\"bank_exposure_z\"].astype(float),\n",
    "    \"Relief_x_util\": wk2[\"Relief\"].astype(float) * wk2[\"dealer_util_z_lag1w\"].astype(float),\n",
    "})\n",
    "for c in [\"VIX\",\"HY_OAS\",\"BAA10Y\",\"SOFR\",\"spr_tgcr\",\"spr_effr\"]:\n",
    "    if c in wk2.columns:\n",
    "        X[c] = pd.to_numeric(wk2[c], errors=\"coerce\")\n",
    "X = pd.concat([X.reset_index(drop=True), fe.reset_index(drop=True)], axis=1)\n",
    "pooled = _nw_ols(pd.to_numeric(wk2[\"dwedge\"], errors=\"coerce\"), X, lags=NW_LAGS_WEEKLY)\n",
    "\n",
    "for nm in [\"Relief\",\"Relief_x_bank\",\"Relief_x_util\"]:\n",
    "    results.append({\n",
    "        \"series_id\": \"__pooled__\",\n",
    "        \"coef\": nm,\n",
    "        \"beta\": pooled.params.get(nm, np.nan),\n",
    "        \"se\": pooled.bse.get(nm, np.nan),\n",
    "    })\n",
    "\n",
    "# by series\n",
    "for sid in sorted(wk[\"series_id\"].unique()):\n",
    "    sub = wk[wk[\"series_id\"]==sid].dropna(subset=[\"dwedge\"]).copy()\n",
    "    if len(sub) < 25:\n",
    "        continue\n",
    "    out = run_mech(sub, y=\"dwedge\", add_controls=True, lags=NW_LAGS_WEEKLY)\n",
    "    out[\"series_id\"] = sid\n",
    "    results.append(out)\n",
    "\n",
    "mech = pd.concat([r if isinstance(r, pd.DataFrame) else pd.DataFrame([r]) for r in results], ignore_index=True)\n",
    "mech[\"lo\"] = mech[\"beta\"] - 1.96*mech[\"se\"]\n",
    "mech[\"hi\"] = mech[\"beta\"] + 1.96*mech[\"se\"]\n",
    "\n",
    "mech_path = OUT_DIR / \"layer2_mechanism_results.csv\"\n",
    "mech.to_csv(mech_path, index=False)\n",
    "\n",
    "print(\"Saved:\", mech_path)\n",
    "mech.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5fafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 — Layer 2 plots: coefficient plot for pooled + top series\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "focus = mech[mech[\"coef\"].isin([\"Relief\",\"Relief_x_bank\",\"Relief_x_util\"])].copy()\n",
    "\n",
    "# Select pooled + first few series (by abs Relief effect)\n",
    "pool = focus[focus[\"series_id\"]==\"__pooled__\"].copy()\n",
    "rest = focus[focus[\"series_id\"]!=\"__pooled__\"].copy()\n",
    "\n",
    "rank = (rest[rest[\"coef\"]==\"Relief\"]\n",
    "        .assign(absb=lambda d: d[\"beta\"].abs())\n",
    "        .sort_values(\"absb\", ascending=False)\n",
    "        .head(8)[\"series_id\"].tolist())\n",
    "\n",
    "plot_df = pd.concat([pool, rest[rest[\"series_id\"].isin(rank)]], ignore_index=True)\n",
    "\n",
    "plot_df[\"label\"] = plot_df[\"series_id\"] + \" | \" + plot_df[\"coef\"]\n",
    "\n",
    "plot_df = plot_df.sort_values([\"coef\",\"series_id\"]).reset_index(drop=True)\n",
    "y = np.arange(len(plot_df))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, max(5, 0.28*len(plot_df))))\n",
    "ax.errorbar(\n",
    "    plot_df[\"beta\"], y,\n",
    "    xerr=[plot_df[\"beta\"]-plot_df[\"lo\"], plot_df[\"hi\"]-plot_df[\"beta\"]],\n",
    "    fmt=\"o\"\n",
    ")\n",
    "ax.axvline(0, linestyle=\"--\", linewidth=1)\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(plot_df[\"label\"])\n",
    "ax.set_title(\"Weekly mechanism coefficients (95% CI): pooled + top series\")\n",
    "ax.set_xlabel(\"Coefficient\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"layer2_mechanism_coeffs.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved:\", FIG_DIR / \"layer2_mechanism_coeffs.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
