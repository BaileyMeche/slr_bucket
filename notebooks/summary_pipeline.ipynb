{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary pipeline: arb\n",
    "a outcomes and event-study design\n",
    "\n",
    "This notebook rebuilds Layer 1 around real `arb_*` outcomes from `tips_treasury_implied_rf_2010` and aligns controls from the repo data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "from slr_bucket.econometrics.event_study import add_event_time, event_study_regression, jump_estimator\n",
    "from slr_bucket.io import build_data_catalog, load_any_table, resolve_dataset_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "CONFIG = {\n",
    "    \"outcomes_source\": \"tips_treasury_implied_rf_2010\",\n",
    "    \"outcome_pattern\": \"arb_\",\n",
    "    \"tenors_required\": [2, 5, 10],\n",
    "    \"events\": [\"2020-04-01\", \"2021-03-19\", \"2021-03-31\"],\n",
    "    \"windows\": [20, 60],\n",
    "    \"event_bins\": [(-60,-41),(-40,-21),(-20,-1),(0,0),(1,20),(21,40),(41,60)],\n",
    "    \"total_controls\": [\"VIX\", \"HY_OAS\", \"BAA10Y\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"],\n",
    "    \"direct_controls\": [\"VIX\", \"HY_OAS\", \"BAA10Y\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\", \"SOFR\", \"spr_tgcr\", \"spr_effr\"],\n",
    "    \"hac_lags\": 5,\n",
    "    \"run_layer2\": True,\n",
    "}\n",
    "cfg_hash = hashlib.sha256(json.dumps(CONFIG, sort_keys=True).encode()).hexdigest()[:12]\n",
    "run_stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "repo_root = Path.cwd()\n",
    "run_dir = repo_root / \"outputs\" / \"summary_pipeline\" / f\"{run_stamp}_{cfg_hash}\"\n",
    "for sub in [\"figures\",\"tables\",\"data\",\"logs\"]:\n",
    "    (run_dir / sub).mkdir(parents=True, exist_ok=True)\n",
    "latest_dir = repo_root / \"outputs\" / \"summary_pipeline\" / \"latest\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\", handlers=[logging.FileHandler(run_dir / \"logs\" / \"pipeline.log\"), logging.StreamHandler()], force=True)\n",
    "logger = logging.getLogger(\"summary_pipeline\")\n",
    "print(run_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data catalog and dataset inventory\n",
    "\n",
    "New `/data` structure uses layered folders (`raw`, `intermediate`, `series`). This run uses:\n",
    "- Outcomes: `data/series/tips_treasury_implied_rf_2010.(parquet|csv)` (`arb_*`).\n",
    "- Preferred merged controls: `data/intermediate/analysis_panel.csv` (if valid for required columns).\n",
    "- Fallback controls from raw inputs:\n",
    "  - `raw/event_inputs/controls_vix_creditspreads_fred`\n",
    "  - `raw/event_inputs/repo_rates_combined` (or `repo_rates_fred`)\n",
    "  - `raw/event_inputs/treasury_issuance_by_tenor_fiscaldata`\n",
    "- Layer 2 proxies (optional):\n",
    "  - `raw/event_inputs/primary_dealer_stats_ofr_stfm_nypd_long`\n",
    "  - `raw/event_inputs/bank_exposure_y9c_agg_daily.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = build_data_catalog(repo_root / \"data\")\n",
    "catalog.to_csv(run_dir / \"data\" / \"data_catalog.csv\", index=False)\n",
    "catalog.to_parquet(run_dir / \"data\" / \"data_catalog.parquet\", index=False)\n",
    "catalog.to_markdown(run_dir / \"data\" / \"data_catalog.md\", index=False)\n",
    "catalog.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcomes: arb_* only\n",
    "outcome_path = resolve_dataset_path(CONFIG[\"outcomes_source\"], expected_dir=repo_root / \"data\" / \"series\")\n",
    "out = load_any_table(outcome_path)\n",
    "out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "arb_cols = sorted([c for c in out.columns if c.startswith(CONFIG[\"outcome_pattern\"])], key=lambda c: int(c.split(\"_\")[1]))\n",
    "if not arb_cols:\n",
    "    raise ValueError(\"No arb_* columns found in outcomes source\")\n",
    "arb_long = out[[\"date\", *arb_cols]].melt(id_vars=[\"date\"], var_name=\"outcome\", value_name=\"y\")\n",
    "arb_long[\"tenor\"] = arb_long[\"outcome\"].str.extract(r\"arb_(\\d+)\").astype(float).astype(\"Int64\")\n",
    "arb_long = arb_long.dropna(subset=[\"date\",\"y\",\"tenor\"]).sort_values([\"tenor\",\"date\"]).reset_index(drop=True)\n",
    "val_abs_q = arb_long[\"y\"].abs().quantile([0.5,0.9,0.99]).to_dict()\n",
    "unit_note = \"Values look like bps\" if val_abs_q.get(0.5,0) > 0.2 else \"Values look like decimals\"\n",
    "logger.info(\"Loaded outcomes from %s with tenors=%s\", outcome_path, sorted(arb_long[\"tenor\"].dropna().unique().tolist()))\n",
    "{\"outcome_path\": str(outcome_path), \"arb_columns\": arb_cols, \"value_quantiles_abs\": val_abs_q, \"unit_note\": unit_note}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls: prefer intermediate analysis_panel if valid, else fallback build from raw.\n",
    "def build_controls_panel():\n",
    "    needed = set(CONFIG[\"direct_controls\"])\n",
    "    try:\n",
    "        p = resolve_dataset_path(\"analysis_panel\", expected_dir=repo_root / \"data\" / \"intermediate\")\n",
    "        panel = load_any_table(p)\n",
    "        panel[\"date\"] = pd.to_datetime(panel[\"date\"], errors=\"coerce\")\n",
    "        if needed.issubset(set(panel.columns)):\n",
    "            logger.info(\"Using controls from intermediate analysis_panel: %s\", p)\n",
    "            return panel[[\"date\", *sorted(needed)]].copy(), str(p)\n",
    "    except Exception as exc:\n",
    "        logger.warning(\"analysis_panel unavailable/invalid (%s), using raw fallback\", exc)\n",
    "\n",
    "    fred = load_any_table(resolve_dataset_path(\"controls_vix_creditspreads_fred\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    fred[\"date\"] = pd.to_datetime(fred[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    try:\n",
    "        repo = load_any_table(resolve_dataset_path(\"repo_rates_combined\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    except FileNotFoundError:\n",
    "        repo = load_any_table(resolve_dataset_path(\"repo_rates_fred\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    repo[\"date\"] = pd.to_datetime(repo[\"date\"], errors=\"coerce\")\n",
    "    repo = repo.rename(columns={\"TGCR\":\"tgcr\", \"EFFR\":\"effr\"})\n",
    "    if \"spr_tgcr\" not in repo.columns and {\"SOFR\",\"tgcr\"}.issubset(repo.columns):\n",
    "        repo[\"spr_tgcr\"] = pd.to_numeric(repo[\"SOFR\"], errors=\"coerce\") - pd.to_numeric(repo[\"tgcr\"], errors=\"coerce\")\n",
    "    if \"spr_effr\" not in repo.columns and {\"SOFR\",\"effr\"}.issubset(repo.columns):\n",
    "        repo[\"spr_effr\"] = pd.to_numeric(repo[\"SOFR\"], errors=\"coerce\") - pd.to_numeric(repo[\"effr\"], errors=\"coerce\")\n",
    "\n",
    "    issu = load_any_table(resolve_dataset_path(\"treasury_issuance_by_tenor_fiscaldata\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    issu[\"date\"] = pd.to_datetime(issu.get(\"issue_date\"), errors=\"coerce\")\n",
    "    issu[\"tenor_bucket\"] = pd.to_numeric(issu[\"tenor_bucket\"], errors=\"coerce\")\n",
    "    issu[\"issuance_amount\"] = pd.to_numeric(issu[\"issuance_amount\"], errors=\"coerce\") / 1e9\n",
    "    d = issu.pivot_table(index=\"date\", columns=\"tenor_bucket\", values=\"issuance_amount\", aggfunc=\"sum\").reset_index()\n",
    "    d = d.rename(columns={7.0:\"issu_7_bil\", 10.0:\"issu_10_bil\", 14.0:\"issu_14_bil\", 20.0:\"issu_20_bil\", 30.0:\"issu_30_bil\"})\n",
    "    if \"issu_14_bil\" not in d.columns:\n",
    "        d[\"issu_14_bil\"] = d.get(\"issu_10_bil\", 0) + d.get(\"issu_20_bil\", 0)\n",
    "\n",
    "    controls = fred.merge(repo[[c for c in [\"date\",\"SOFR\",\"spr_tgcr\",\"spr_effr\"] if c in repo.columns]], on=\"date\", how=\"outer\")                   .merge(d[[c for c in [\"date\",\"issu_7_bil\",\"issu_14_bil\",\"issu_30_bil\"] if c in d.columns]], on=\"date\", how=\"left\")\n",
    "    logger.info(\"Built controls from raw event_inputs\")\n",
    "    return controls, \"raw_event_inputs_fallback\"\n",
    "\n",
    "controls, controls_source = build_controls_panel()\n",
    "controls = controls.sort_values(\"date\")\n",
    "controls.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_long = arb_long.merge(controls, on=\"date\", how=\"left\")\n",
    "for c in CONFIG[\"direct_controls\"]:\n",
    "    if c in panel_long.columns:\n",
    "        panel_long[c] = pd.to_numeric(panel_long[c], errors=\"coerce\")\n",
    "panel_long.to_parquet(run_dir / \"data\" / \"arb_panel_long.parquet\", index=False)\n",
    "panel_long.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1A: summary stats by tenor and regime\n",
    "regimes = {\n",
    "    \"pre\": (pd.Timestamp(\"2019-01-01\"), pd.Timestamp(\"2020-03-31\")),\n",
    "    \"relief\": (pd.Timestamp(\"2020-04-01\"), pd.Timestamp(\"2021-03-31\")),\n",
    "    \"post\": (pd.Timestamp(\"2021-04-01\"), pd.Timestamp.max),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for tenor, g in panel_long.groupby(\"tenor\"):\n",
    "    g = g.sort_values(\"date\")\n",
    "    for regime, (start, end) in regimes.items():\n",
    "        s = g[(g[\"date\"]>=start) & (g[\"date\"]<=end)][\"y\"].dropna()\n",
    "        if s.empty:\n",
    "            continue\n",
    "        lb_p = np.nan\n",
    "        try:\n",
    "            lb = acorr_ljungbox(s, lags=[min(10, max(1, len(s)//5))], return_df=True)\n",
    "            lb_p = float(lb[\"lb_pvalue\"].iloc[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "        rows.append({\n",
    "            \"tenor\": int(tenor), \"regime\": regime,\n",
    "            \"sample_start\": s.index.min(), \"sample_end\": s.index.max(), \"N\": int(s.shape[0]),\n",
    "            \"mean\": s.mean(), \"std\": s.std(),\n",
    "            \"p1\": s.quantile(0.01), \"p5\": s.quantile(0.05), \"p50\": s.quantile(0.5), \"p95\": s.quantile(0.95), \"p99\": s.quantile(0.99),\n",
    "            \"autocorr1\": s.autocorr(lag=1), \"ljungbox_pvalue\": lb_p,\n",
    "        })\n",
    "summary_stats = pd.DataFrame(rows)\n",
    "summary_stats.to_csv(run_dir / \"tables\" / \"summary_stats.csv\", index=False)\n",
    "summary_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1B: jump regressions (TOTAL vs DIRECT)\n",
    "jump_rows = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for w in CONFIG[\"windows\"]:\n",
    "        for tenor, g in panel_long.groupby(\"tenor\"):\n",
    "            for spec, controls_list in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "                est, se, n = jump_estimator(g, y_col=\"y\", event_date=event, window=w, controls=controls_list, hac_lags=CONFIG[\"hac_lags\"])\n",
    "                jump_rows.append({\n",
    "                    \"event\": event, \"window\": w, \"tenor\": int(tenor), \"spec\": spec,\n",
    "                    \"estimate\": est, \"se\": se, \"ci_low\": est - 1.96*se if pd.notna(est) and pd.notna(se) else np.nan,\n",
    "                    \"ci_high\": est + 1.96*se if pd.notna(est) and pd.notna(se) else np.nan, \"N\": n,\n",
    "                })\n",
    "jump_results = pd.DataFrame(jump_rows)\n",
    "jump_results.to_csv(run_dir / \"tables\" / \"jump_results.csv\", index=False)\n",
    "jump_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1C: binned event-study + plots\n",
    "import matplotlib.pyplot as plt\n",
    "bin_rows = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for tenor, g in panel_long.groupby(\"tenor\"):\n",
    "        for spec, controls_list in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "            es = event_study_regression(g, y_col=\"y\", event_date=event, bins=CONFIG[\"event_bins\"], controls=controls_list, hac_lags=CONFIG[\"hac_lags\"])\n",
    "            if es.empty:\n",
    "                continue\n",
    "            es[\"event\"] = event\n",
    "            es[\"tenor\"] = int(tenor)\n",
    "            es[\"spec\"] = spec\n",
    "            bin_rows.append(es)\n",
    "            p = es.copy().sort_values(\"term\")\n",
    "            fig, ax = plt.subplots(figsize=(8,4))\n",
    "            ax.plot(p[\"term\"], p[\"estimate\"], marker=\"o\")\n",
    "            ax.fill_between(p[\"term\"], p[\"ci_low\"], p[\"ci_high\"], alpha=0.2)\n",
    "            ax.axhline(0, color=\"black\", lw=1)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.set_title(f\"Event study path | event={event} tenor={int(tenor)} spec={spec}\")\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(run_dir / \"figures\" / f\"event_path_arb_{int(tenor)}y_{event}_{spec.lower()}.png\", dpi=150)\n",
    "            plt.close(fig)\n",
    "eventstudy_bins = pd.concat(bin_rows, ignore_index=True) if bin_rows else pd.DataFrame()\n",
    "eventstudy_bins.to_csv(run_dir / \"tables\" / \"eventstudy_bins.csv\", index=False)\n",
    "eventstudy_bins.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1D pooled regression with tenor FE + stargazer export\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "pooled_rows = []\n",
    "stargazer_models = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    work = add_event_time(panel_long, event)\n",
    "    work = work[work[\"event_time\"].between(-60, 60)].copy()\n",
    "    work[\"post\"] = (work[\"event_time\"] >= 0).astype(int)\n",
    "    for spec, controls_list in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "        use_cols = [\"y\",\"post\",\"tenor\", *[c for c in controls_list if c in work.columns]]\n",
    "        reg = work[use_cols].dropna().copy()\n",
    "        if reg.empty:\n",
    "            continue\n",
    "        rhs = \"post + C(tenor)\"\n",
    "        if len(use_cols) > 3:\n",
    "            rhs += \" + \" + \" + \".join([c for c in use_cols if c not in {\"y\",\"post\",\"tenor\"}])\n",
    "        res = ols(f\"y ~ {rhs}\", data=reg).fit()\n",
    "        robust = res.get_robustcov_results(cov_type=\"HAC\", maxlags=CONFIG[\"hac_lags\"])\n",
    "        stargazer_models.append(robust)\n",
    "        post_idx = robust.model.exog_names.index(\"post\") if \"post\" in robust.model.exog_names else None\n",
    "        pooled_rows.append({\"event\": event, \"spec\": spec, \"N\": int(robust.nobs), \"post\": robust.params[post_idx] if post_idx is not None else np.nan, \"se\": robust.bse[post_idx] if post_idx is not None else np.nan})\n",
    "\n",
    "pooled_table = pd.DataFrame(pooled_rows)\n",
    "pooled_table.to_csv(run_dir / \"tables\" / \"pooled_jump_results.csv\", index=False)\n",
    "\n",
    "html_out = run_dir / \"tables\" / \"regression_table.html\"\n",
    "try:\n",
    "    from stargazer.stargazer import Stargazer\n",
    "    if stargazer_models:\n",
    "        sg = Stargazer(stargazer_models)\n",
    "        sg.title(\"Pooled jump regressions (HAC SE)\")\n",
    "        html_out.write_text(sg.render_html(), encoding=\"utf-8\")\n",
    "    else:\n",
    "        html_out.write_text(\"<html><body><p>No pooled models available.</p></body></html>\", encoding=\"utf-8\")\n",
    "except Exception as exc:\n",
    "    html_out.write_text(f\"<html><body><p>Stargazer unavailable: {exc}</p></body></html>\", encoding=\"utf-8\")\n",
    "\n",
    "pooled_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2 mechanism (weekly), skip gracefully if required data missing\n",
    "layer2_note = \"\"\n",
    "try:\n",
    "    pd_long = load_any_table(resolve_dataset_path(\"primary_dealer_stats_ofr_stfm_nypd_long\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    bank = load_any_table(resolve_dataset_path(\"bank_exposure_y9c_agg_daily\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "\n",
    "    pd_long[\"date\"] = pd.to_datetime(pd_long[\"date\"], errors=\"coerce\")\n",
    "    bank[\"date\"] = pd.to_datetime(bank[\"date\"], errors=\"coerce\")\n",
    "    pd_w = pd_long.pivot_table(index=\"date\", columns=\"mnemonic\", values=\"value\", aggfunc=\"mean\").resample(\"W-FRI\").mean()\n",
    "    pd_w[\"utilization_index\"] = pd_w.sum(axis=1, min_count=1)\n",
    "    pd_w[\"utilization_lag1w\"] = pd_w[\"utilization_index\"].shift(1)\n",
    "\n",
    "    b_w = bank.set_index(\"date\").resample(\"W-FRI\").mean()[[\"agg_exempt_share\"]]\n",
    "    y_w = panel_long.set_index(\"date\").groupby(\"tenor\")[\"y\"].resample(\"W-FRI\").mean().reset_index()\n",
    "    y_w = y_w.groupby(\"date\", as_index=False)[\"y\"].mean().set_index(\"date\")\n",
    "    c_w = panel_long.set_index(\"date\")[CONFIG[\"direct_controls\"]].resample(\"W-FRI\").mean()\n",
    "\n",
    "    mech = y_w.join([b_w, pd_w[[\"utilization_lag1w\"]], c_w], how=\"inner\").dropna()\n",
    "    mech[\"relief\"] = ((mech.index >= \"2020-04-01\") & (mech.index <= \"2021-03-31\")).astype(int)\n",
    "    mech[\"z_exempt\"] = (mech[\"agg_exempt_share\"] - mech[\"agg_exempt_share\"].mean()) / mech[\"agg_exempt_share\"].std()\n",
    "    mech[\"z_util_l1\"] = (mech[\"utilization_lag1w\"] - mech[\"utilization_lag1w\"].mean()) / mech[\"utilization_lag1w\"].std()\n",
    "    mech[\"relief_x_exempt\"] = mech[\"relief\"] * mech[\"z_exempt\"]\n",
    "    mech[\"relief_x_util\"] = mech[\"relief\"] * mech[\"z_util_l1\"]\n",
    "\n",
    "    xcols = [\"relief\", \"relief_x_exempt\", \"relief_x_util\", *CONFIG[\"direct_controls\"]]\n",
    "    reg = mech[[\"y\", *xcols]].dropna()\n",
    "    X = sm.add_constant(reg[xcols], has_constant=\"add\")\n",
    "    res = sm.OLS(reg[\"y\"], X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\":2})\n",
    "    mech_out = pd.DataFrame({\"term\":res.params.index, \"coef\":res.params.values, \"se\":res.bse.values})\n",
    "    mech_out.to_csv(run_dir / \"tables\" / \"layer2_mechanism_weekly.csv\", index=False)\n",
    "    layer2_note = \"Layer 2 executed successfully.\"\n",
    "except Exception as exc:\n",
    "    layer2_note = f\"Layer 2 skipped gracefully due to missing/unusable inputs: {exc}\"\n",
    "\n",
    "print(layer2_note)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh latest and write run metadata\n",
    "if latest_dir.exists():\n",
    "    shutil.rmtree(latest_dir)\n",
    "shutil.copytree(run_dir, latest_dir)\n",
    "\n",
    "notes = {\n",
    "    \"run_dir\": str(run_dir),\n",
    "    \"controls_source\": controls_source,\n",
    "    \"outcomes_source\": str(outcome_path),\n",
    "    \"arb_columns\": arb_cols,\n",
    "    \"unit_note\": unit_note,\n",
    "    \"layer2_note\": layer2_note,\n",
    "}\n",
    "(run_dir / \"README.md\").write_text(\"# Summary pipeline run\n",
    "\n",
    "```json\n",
    "\" + json.dumps(notes, indent=2) + \"\n",
    "```\n",
    "\", encoding=\"utf-8\")\n",
    "notes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}