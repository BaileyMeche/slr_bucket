{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLR summary pipeline\n",
    "\n",
    "Run-all reproducible pipeline for daily_long event-study analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from slr_bucket.config import PipelineConfig\n",
    "\n",
    "# CONFIG\n",
    "CONFIG = PipelineConfig(\n",
    "    event_dates=['2020-04-01', '2021-03-19', '2021-03-31'],\n",
    "    windows=[3, 5, 10],\n",
    "    event_bins=[(-60,-41),(-40,-21),(-20,-1),(0,0),(1,20),(21,40),(41,60)],\n",
    "    dependent_series=None,\n",
    "    tenor_subset=None,\n",
    "    total_controls=[],\n",
    "    direct_controls=['sofr', 'tgcr', 'bgcr'],\n",
    "    hac_lags=5,\n",
    "    bootstrap_reps=200,\n",
    "    bootstrap_block_size=5,\n",
    "    random_seed=42,\n",
    "    output_root='outputs/summary_pipeline',\n",
    "    cache_root='outputs/cache',\n",
    ")\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "SRC_DIR = REPO_ROOT / 'src'\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "DATA_DIR = REPO_ROOT / 'data'\n",
    "print(json.dumps(CONFIG.__dict__, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inputs and construction of daily_long\n",
    "\n",
    "This section discovers local files, builds a data catalog, validates `daily_long`, and persists deterministic outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from slr_bucket.io import build_data_catalog, find_daily_long, discover_funding_series\n",
    "from slr_bucket.pipeline import prepare_run_dirs, refresh_latest, setup_logging, write_catalog_outputs, write_run_readme\n",
    "from slr_bucket.validation import validate_daily_long\n",
    "\n",
    "run_dirs = prepare_run_dirs(REPO_ROOT, CONFIG)\n",
    "setup_logging(run_dirs['logs'] / 'pipeline.log')\n",
    "logger = logging.getLogger('summary_pipeline')\n",
    "\n",
    "catalog = build_data_catalog(DATA_DIR)\n",
    "write_catalog_outputs(catalog, run_dirs['data'])\n",
    "logger.info('catalog rows=%s', len(catalog))\n",
    "\n",
    "funding_mapping = discover_funding_series(DATA_DIR)\n",
    "logger.info('funding mapping: %s', funding_mapping)\n",
    "\n",
    "daily_long = validate_daily_long(find_daily_long(DATA_DIR))\n",
    "cache_dir = REPO_ROOT / CONFIG.cache_root\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "cache_path = cache_dir / f'daily_long_{CONFIG.to_hash()}.parquet'\n",
    "daily_long.to_parquet(cache_path, index=False)\n",
    "logger.info('daily_long rows=%s saved=%s', len(daily_long), cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = daily_long.pivot_table(index=['date','tenor'], columns='series', values='value', aggfunc='last').reset_index()\n",
    "if CONFIG.tenor_subset:\n",
    "    pivot = pivot[pivot['tenor'].isin(CONFIG.tenor_subset)]\n",
    "\n",
    "series_candidates = [c for c in pivot.columns if c not in {'date', 'tenor'}]\n",
    "if CONFIG.dependent_series:\n",
    "    dep_series = [s for s in CONFIG.dependent_series if s in series_candidates]\n",
    "else:\n",
    "    dep_series = series_candidates[: min(len(series_candidates), 5)]\n",
    "\n",
    "if not dep_series:\n",
    "    raise ValueError('No dependent series selected. Check CONFIG.dependent_series or daily_long series availability.')\n",
    "\n",
    "logger.info('dependent series: %s', dep_series)\n",
    "logger.info('tenors: %s', sorted(pivot['tenor'].dropna().astype(str).unique().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Econometric designs\n",
    "\n",
    "- Windowed jumps: pre/post mean-shift regression with HAC SE and bootstrap robustness.\n",
    "- Event-study bins: total-effect and direct-effect specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from slr_bucket.econometrics.event_study import jump_estimator, block_bootstrap_jump, event_study_regression\n",
    "from slr_bucket.plotting.plots import plot_series_with_events, plot_event_paths\n",
    "\n",
    "jump_rows = []\n",
    "bin_rows = []\n",
    "for tenor, tdf in pivot.groupby('tenor'):\n",
    "    tdf = tdf.sort_values('date').copy()\n",
    "    for y in dep_series:\n",
    "        if y not in tdf.columns:\n",
    "            continue\n",
    "        sub = tdf[['date', y] + [c for c in CONFIG.direct_controls if c in tdf.columns]].copy()\n",
    "        sub = sub.dropna(subset=[y])\n",
    "        if sub.empty:\n",
    "            warnings.warn(f'No data for {tenor}-{y}')\n",
    "            continue\n",
    "\n",
    "        plot_series_with_events(sub.rename(columns={y: 'dep'}), 'dep', CONFIG.event_dates, f'{tenor} {y}', run_dirs['figures'] / f'series_{tenor}_{y}.png')\n",
    "\n",
    "        for event in CONFIG.event_dates:\n",
    "            for w in CONFIG.windows:\n",
    "                est, se, n = jump_estimator(sub, y, event, w, controls=None, hac_lags=CONFIG.hac_lags)\n",
    "                bse = block_bootstrap_jump(sub, y, event, w, controls=None, reps=CONFIG.bootstrap_reps, block_size=CONFIG.bootstrap_block_size, seed=CONFIG.random_seed)\n",
    "                jump_rows.append({'event_date': event, 'tenor': tenor, 'series': y, 'window': w, 'spec': 'total', 'estimate': est, 'se': se, 'bootstrap_se': bse, 'ci_low': est-1.96*se if np.isfinite(est) and np.isfinite(se) else np.nan, 'ci_high': est+1.96*se if np.isfinite(est) and np.isfinite(se) else np.nan, 'n': n})\n",
    "\n",
    "                est_d, se_d, n_d = jump_estimator(sub, y, event, w, controls=CONFIG.direct_controls, hac_lags=CONFIG.hac_lags)\n",
    "                jump_rows.append({'event_date': event, 'tenor': tenor, 'series': y, 'window': w, 'spec': 'direct', 'estimate': est_d, 'se': se_d, 'bootstrap_se': np.nan, 'ci_low': est_d-1.96*se_d if np.isfinite(est_d) and np.isfinite(se_d) else np.nan, 'ci_high': est_d+1.96*se_d if np.isfinite(est_d) and np.isfinite(se_d) else np.nan, 'n': n_d})\n",
    "\n",
    "            for spec_name, controls in [('total', None), ('direct', CONFIG.direct_controls)]:\n",
    "                bins_df = event_study_regression(sub, y, event, CONFIG.event_bins, controls=controls, hac_lags=CONFIG.hac_lags)\n",
    "                bins_df['event_date'] = event\n",
    "                bins_df['tenor'] = tenor\n",
    "                bins_df['series'] = y\n",
    "                bins_df['spec'] = spec_name\n",
    "                bin_rows.append(bins_df)\n",
    "                plot_event_paths(bins_df, f'{tenor} {y} {event} {spec_name}', run_dirs['figures'] / f'event_path_{tenor}_{y}_{event}_{spec_name}.png')\n",
    "\n",
    "jump_table = pd.DataFrame(jump_rows)\n",
    "bin_table = pd.concat(bin_rows, ignore_index=True) if bin_rows else pd.DataFrame()\n",
    "\n",
    "jump_table.to_csv(run_dirs['tables'] / 'jump_estimates.csv', index=False)\n",
    "if not bin_table.empty:\n",
    "    bin_table.to_csv(run_dirs['tables'] / 'event_study_bins.csv', index=False)\n",
    "\n",
    "jump_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility + how to rerun\n",
    "\n",
    "- Run all cells in Jupyter, or execute `python scripts/run_notebook.py`.\n",
    "- Outputs are timestamped + config-hashed, and `latest/` is refreshed each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = (\n",
    "    f\"Processed {len(daily_long)} daily_long rows across {daily_long['tenor'].nunique()} tenors and \"\n",
    "    f\"{daily_long['series'].nunique()} series.\"\n",
    ")\n",
    "write_run_readme(run_dirs['run'], CONFIG, notes)\n",
    "print('Run dir:', run_dirs['run'])\n",
    "print('Latest:', REPO_ROOT / CONFIG.output_root / 'latest')\n",
    "latest_dir = refresh_latest(REPO_ROOT, CONFIG, run_dirs['run'])\n",
    "print('Latest refreshed:', latest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks\n",
    "\n",
    "Basic diagnostics for missingness and sample support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity = (\n",
    "    daily_long.groupby(['tenor','series'])['value']\n",
    "    .agg(['count','mean','std'])\n",
    "    .reset_index()\n",
    "    .sort_values('count', ascending=False)\n",
    ")\n",
    "sanity.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}