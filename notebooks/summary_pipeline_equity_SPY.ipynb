{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary pipeline (multi-tenor arb outcomes)\n",
    "This notebook runs the arb outcome event-study pipeline using real repo data only (no synthetic seeds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Repo path hygiene (template behavior preserved, but robust if run from different working dirs)\n",
    "sys.path.insert(2, \"../src\")\n",
    "# If running inside a /src directory, step back to repo root\n",
    "try:\n",
    "    if \"src\" in os.getcwd().replace(\"\\\\\",\"/\").split(\"/\"):\n",
    "        os.chdir(os.path.pardir)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Try to import project utilities; fall back to local minimal implementations if unavailable ---\n",
    "try:\n",
    "    from slr_bucket.econometrics.event_study import add_event_time, event_study_regression, jump_estimator\n",
    "    from slr_bucket.io import build_data_catalog, load_any_table, resolve_dataset_path, as_daily_date, coerce_num\n",
    "except Exception as _imp_exc:  # noqa: BLE001\n",
    "    logging.getLogger(__name__).warning(\"slr_bucket imports unavailable (%s). Using local fallbacks.\", _imp_exc)\n",
    "\n",
    "    def as_daily_date(s):\n",
    "        return pd.to_datetime(s, errors=\"coerce\").dt.floor(\"D\")\n",
    "\n",
    "    def coerce_num(x):\n",
    "        return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "    def resolve_dataset_path(name: str, expected_dir: Path | None = None) -> Path:\n",
    "        # Minimal: treat `name` as path if it exists; else look under expected_dir for common extensions.\n",
    "        p = Path(name)\n",
    "        if p.exists():\n",
    "            return p\n",
    "        if expected_dir is None:\n",
    "            expected_dir = Path.cwd()\n",
    "        for ext in (\".csv\", \".parquet\"):\n",
    "            cand = expected_dir / f\"{name}{ext}\"\n",
    "            if cand.exists():\n",
    "                return cand\n",
    "        raise FileNotFoundError(f\"Could not resolve dataset path for {name} under {expected_dir}\")\n",
    "\n",
    "    def load_any_table(path: Path | str) -> pd.DataFrame:\n",
    "        path = Path(path)\n",
    "        if path.suffix.lower() == \".csv\":\n",
    "            return pd.read_csv(path)\n",
    "        if path.suffix.lower() == \".parquet\":\n",
    "            # Parquet requires pyarrow/fastparquet; raise a helpful error\n",
    "            raise ImportError(\"Parquet read requires pyarrow/fastparquet in this environment.\")\n",
    "        raise ValueError(f\"Unsupported file type: {path.suffix}\")\n",
    "\n",
    "    def build_data_catalog(root: Path) -> pd.DataFrame:\n",
    "        # Lightweight catalog: list csv files under root\n",
    "        rows=[]\n",
    "        for p in root.rglob(\"*.csv\"):\n",
    "            try:\n",
    "                df=pd.read_csv(p, nrows=2)\n",
    "                cols=\",\".join(df.columns.astype(str))\n",
    "                rows.append({\"path\": str(p), \"layer\": p.parent.name, \"rows\": None, \"columns\": cols,\n",
    "                             \"frequency\": None, \"date_min\": None, \"date_max\": None, \"key_columns\": \"date\",\n",
    "                             \"join_hints\": \"\"})\n",
    "            except Exception:\n",
    "                rows.append({\"path\": str(p), \"layer\": p.parent.name, \"rows\": None, \"columns\": None,\n",
    "                             \"frequency\": None, \"date_min\": None, \"date_max\": None, \"key_columns\": \"date\",\n",
    "                             \"join_hints\": \"\"})\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def add_event_time(df: pd.DataFrame, event_date: str) -> pd.DataFrame:\n",
    "        out=df.copy()\n",
    "        t0=pd.Timestamp(event_date)\n",
    "        out[\"event_time\"]=(pd.to_datetime(out[\"date\"]) - t0).dt.days\n",
    "        return out\n",
    "\n",
    "    def _nw_ols(y: pd.Series, X: pd.DataFrame, lags: int):\n",
    "        Xc = sm.add_constant(X, has_constant=\"add\")\n",
    "        # Coerce to numeric to avoid object-dtype issues\n",
    "        Xc = Xc.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
    "        yv = pd.to_numeric(y, errors=\"coerce\")\n",
    "        reg = pd.concat([yv.rename(\"y\"), Xc], axis=1).dropna()\n",
    "        if reg.empty:\n",
    "            raise ValueError(\"Empty regression sample after numeric coercion/dropna.\")\n",
    "        res = sm.OLS(reg[\"y\"], reg.drop(columns=[\"y\"]), missing=\"drop\").fit()\n",
    "        robust = res.get_robustcov_results(cov_type=\"HAC\", maxlags=lags)\n",
    "        return robust\n",
    "\n",
    "    def jump_estimator(df: pd.DataFrame, y_col: str, event_date: str, window: int, controls: list[str], hac_lags: int):\n",
    "        sub = add_event_time(df, event_date)\n",
    "        sub = sub[sub[\"event_time\"].between(-window, window)].copy()\n",
    "        sub[\"post\"] = (sub[\"event_time\"] >= 0).astype(int)\n",
    "        use_controls=[c for c in controls if c in sub.columns]\n",
    "        cols=[y_col,\"post\",*use_controls]\n",
    "        reg=sub[cols].dropna()\n",
    "        if reg.empty:\n",
    "            return (np.nan, np.nan, 0)\n",
    "        X=reg[[\"post\",*use_controls]]\n",
    "        y=reg[y_col]\n",
    "        robust=_nw_ols(y, X, lags=hac_lags)\n",
    "        i=robust.model.exog_names.index(\"post\")\n",
    "        return (float(robust.params[i]), float(robust.bse[i]), int(robust.nobs))\n",
    "\n",
    "    def event_study_regression(df: pd.DataFrame, y_col: str, event_date: str, bins: list[tuple[int,int]], controls: list[str], hac_lags: int):\n",
    "        sub = add_event_time(df, event_date)\n",
    "        sub = sub[sub[\"event_time\"].between(min(a for a,b in bins), max(b for a,b in bins))].copy()\n",
    "        # Assign bins\n",
    "        def _assign_bin(k):\n",
    "            for a,b in bins:\n",
    "                if a<=k<=b: return f\"bin_[{a},{b}]\"\n",
    "            return np.nan\n",
    "        sub[\"bin\"]=sub[\"event_time\"].apply(_assign_bin)\n",
    "        # Omit baseline bin if present\n",
    "        baseline = \"bin_[-20,-1]\" if \"bin_[-20,-1]\" in sub[\"bin\"].unique().tolist() else None\n",
    "        dummies=pd.get_dummies(sub[\"bin\"])\n",
    "        if baseline and baseline in dummies.columns:\n",
    "            dummies=dummies.drop(columns=[baseline])\n",
    "        use_controls=[c for c in controls if c in sub.columns]\n",
    "        reg=pd.concat([sub[[y_col]], dummies, sub[use_controls]], axis=1).dropna()\n",
    "        if reg.empty or dummies.shape[1]==0:\n",
    "            return pd.DataFrame()\n",
    "        y=reg[y_col]\n",
    "        X=reg.drop(columns=[y_col])\n",
    "        robust=_nw_ols(y, X, lags=hac_lags)\n",
    "        out=[]\n",
    "        for term in dummies.columns:\n",
    "            if term in robust.model.exog_names:\n",
    "                j=robust.model.exog_names.index(term)\n",
    "                est=float(robust.params[j]); se=float(robust.bse[j])\n",
    "                out.append({\"term\": term, \"estimate\": est, \"se\": se,\n",
    "                            \"ci_low\": est-1.96*se, \"ci_high\": est+1.96*se, \"n\": int(robust.nobs)})\n",
    "        return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9]+\", \"_\", s).strip(\"_\")\n",
    "    return s[:80] if s else \"series\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"outcomes_file\": \"data/series/equity_spot_spread_SPY.csv\",\n",
    "    \"series_columns\": [\"spread_SPY_filtered\"],\n",
    "    \"series_kind\": \"single\",\n",
    "    \"series_label\": \"Equity spotâ€“futures (SPY) arbitrage\",\n",
    "    \"events\": [\"2020-04-01\", \"2021-03-19\", \"2021-03-31\"],\n",
    "    \"windows\": [20, 60],\n",
    "    \"event_bins\": [(-60, -41), (-40, -21), (-20, -1), (0, 0), (1, 20), (21, 40), (41, 60)],\n",
    "    \"total_controls\": [\"VIX\", \"HY_OAS\", \"BAA10Y\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"],\n",
    "    \"direct_controls\": [\"VIX\", \"HY_OAS\", \"BAA10Y\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\", \"SOFR\", \"spr_tgcr\", \"spr_effr\"],\n",
    "    \"hac_lags\": 5,\n",
    "}\n",
    "repo_root = Path.cwd().parent\n",
    "cfg_hash = hashlib.sha256(json.dumps(CONFIG, sort_keys=True).encode()).hexdigest()[:12]\n",
    "run_stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = repo_root / \"outputs\" / \"summary_pipeline\" / f\"{run_stamp}_{cfg_hash}\"\n",
    "for sub in [\"figures\", \"tables\", \"data\", \"logs\"]:\n",
    "    (run_dir / sub).mkdir(parents=True, exist_ok=True)\n",
    "latest_dir = repo_root / \"outputs\" / \"summary_pipeline\" / \"latest\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\", handlers=[logging.FileHandler(run_dir / \"logs\" / \"pipeline.log\"), logging.StreamHandler()], force=True)\n",
    "logger = logging.getLogger(\"summary_pipeline_multi\")\n",
    "run_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data map for the new `/data` structure\n",
    "Used in this run:\n",
    "- Outcomes: `data/series/tips_treasury_implied_rf_2010.(parquet|csv)` (`arb_*` only).\n",
    "- Controls (preferred): `data/intermediate/analysis_panel.csv` if all required columns are present.\n",
    "- Controls (fallback):\n",
    "  - `data/raw/event_inputs/controls_vix_creditspreads_fred.(parquet|csv)`\n",
    "  - `data/raw/event_inputs/repo_rates_combined.(parquet|csv)` or `repo_rates_fred`\n",
    "  - `data/raw/event_inputs/treasury_issuance_by_tenor_fiscaldata.(parquet|csv)`\n",
    "- Mechanism proxies (optional):\n",
    "  - `primary_dealer_stats_ofr_stfm_nypd_long`\n",
    "  - `bank_exposure_y9c_agg_daily`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = build_data_catalog(repo_root / \"data\")\n",
    "catalog.to_csv(run_dir / \"data\" / \"data_catalog.csv\", index=False)\n",
    "try:\n",
    "    catalog.to_parquet(run_dir / \"data\" / \"data_catalog.parquet\", index=False)\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    (run_dir / \"data\" / \"data_catalog.parquet.error.txt\").write_text(str(exc), encoding=\"utf-8\")\n",
    "catalog.to_markdown(run_dir / \"data\" / \"data_catalog.md\", index=False)\n",
    "catalog.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load outcome series (template: produce long panel with columns: date, series, y)\n",
    "outcome_path = (repo_root / CONFIG[\"outcomes_file\"]).resolve()\n",
    "outcomes = pd.read_csv(outcome_path)\n",
    "outcomes[\"date\"] = pd.to_datetime(outcomes[\"date\"], errors=\"coerce\")\n",
    "series_cols = [c for c in CONFIG[\"series_columns\"] if c in outcomes.columns]\n",
    "missing = sorted(set(CONFIG[\"series_columns\"]) - set(series_cols))\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected series columns: {missing}\")\n",
    "\n",
    "long = outcomes[[\"date\", *series_cols]].melt(id_vars=[\"date\"], var_name=\"series\", value_name=\"y\")\n",
    "long[\"y\"] = pd.to_numeric(long[\"y\"], errors=\"coerce\")\n",
    "\n",
    "# Series labeling / ordering helper\n",
    "def _series_key(s: str):\n",
    "    s = str(s)\n",
    "    # Try to extract tenor from patterns like *_2Y, *_5Y, arb_2, etc.\n",
    "    m = re.search(r\"(\\d+)\\s*Y\\b\", s)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    m = re.search(r\"arb_(\\d+)\", s)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return s  # fallback string\n",
    "\n",
    "long[\"series_key\"] = long[\"series\"].apply(_series_key)\n",
    "long = long.dropna(subset=[\"date\", \"y\"]).sort_values([\"series_key\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Unit sanity check\n",
    "value_q = long[\"y\"].abs().quantile([0.5, 0.9, 0.99]).to_dict()\n",
    "unit_note = \"Likely basis points\" if value_q.get(0.5, 0) > 0.2 else \"Likely decimal units\"\n",
    "{\"outcome_path\": str(outcome_path), \"series_cols\": series_cols, \"value_q\": value_q, \"unit_note\": unit_note}\n",
    "\n",
    "arb_long = long[[\"date\",\"series\",\"y\",\"series_key\"]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls: prefer intermediate analysis_panel if valid, else fallback build from raw.\n",
    "# def build_controls_panel():\n",
    "\n",
    "needed = set(CONFIG[\"direct_controls\"])\n",
    "try:\n",
    "    p = resolve_dataset_path(\"analysis_panel\", expected_dir=repo_root / \"data\" / \"intermediate\")\n",
    "    panel = load_any_table(p)\n",
    "    panel[\"date\"] = pd.to_datetime(panel[\"date\"], errors=\"coerce\")\n",
    "    if needed.issubset(set(panel.columns)):\n",
    "        logger.info(\"Using controls from intermediate analysis_panel: %s\", p)\n",
    "        controls =  panel[[\"date\", *sorted(needed)]].copy() #, str(p)\n",
    "except Exception as exc:\n",
    "    logger.warning(\"analysis_panel unavailable/invalid (%s), using raw fallback\", exc)\n",
    "\n",
    "fred = load_any_table(resolve_dataset_path(\"controls_vix_creditspreads_fred\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "fred[\"date\"] = pd.to_datetime(fred[\"date\"], errors=\"coerce\")\n",
    "fred[\"date\"] = as_daily_date(fred[\"date\"])\n",
    "try:\n",
    "    repo = load_any_table(resolve_dataset_path(\"repo_rates_combined\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "except FileNotFoundError:\n",
    "    repo = load_any_table(resolve_dataset_path(\"repo_rates_fred\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "repo[\"date\"] = pd.to_datetime(repo[\"date\"], errors=\"coerce\")\n",
    "repo[\"date\"] = as_daily_date(repo[\"date\"])\n",
    "repo = repo.rename(columns={\"TGCR\":\"tgcr\", \"EFFR\":\"effr\"})\n",
    "if \"spr_tgcr\" not in repo.columns and {\"SOFR\",\"tgcr\"}.issubset(repo.columns):\n",
    "    repo[\"spr_tgcr\"] = pd.to_numeric(repo[\"tgcr\"], errors=\"coerce\") - pd.to_numeric(repo[\"SOFR\"], errors=\"coerce\")\n",
    "if \"spr_effr\" not in repo.columns and {\"SOFR\",\"effr\"}.issubset(repo.columns):\n",
    "    repo[\"spr_effr\"] = pd.to_numeric(repo[\"effr\"], errors=\"coerce\") - pd.to_numeric(repo[\"SOFR\"], errors=\"coerce\")\n",
    "\n",
    "issu = load_any_table(resolve_dataset_path(\"treasury_issuance_by_tenor_fiscaldata\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "issu[\"date\"] = pd.to_datetime(issu.get(\"issue_date\"), errors=\"coerce\")\n",
    "issu[\"date\"] = as_daily_date(issu[\"date\"])\n",
    "issu[\"tenor_bucket\"] = pd.to_numeric(issu[\"tenor_bucket\"], errors=\"coerce\")\n",
    "issu[\"issuance_amount\"] = pd.to_numeric(issu[\"issuance_amount\"], errors=\"coerce\") / 1e9\n",
    "d = issu.pivot_table(index=\"date\", columns=\"tenor_bucket\", values=\"issuance_amount\", aggfunc=\"sum\").reset_index()\n",
    "\n",
    "# Robustly rename tenor-bucket columns to issu_*_bil (handles int/float/str column labels)\n",
    "rename_map = {}\n",
    "for col in d.columns:\n",
    "    if col == \"date\":\n",
    "        continue\n",
    "    try:\n",
    "        v = float(col)\n",
    "    except Exception:\n",
    "        continue\n",
    "    if abs(v - 7.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_7_bil\"\n",
    "    elif abs(v - 10.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_10_bil\"\n",
    "    elif abs(v - 14.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_14_bil\"\n",
    "    elif abs(v - 20.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_20_bil\"\n",
    "    elif abs(v - 30.0) < 1e-9:\n",
    "        rename_map[col] = \"issu_30_bil\"\n",
    "d = d.rename(columns=rename_map)\n",
    "\n",
    "# Ensure required issuance controls exist (zeros if not present in file)\n",
    "for c in [\"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\", \"issu_10_bil\", \"issu_20_bil\"]:\n",
    "    if c not in d.columns:\n",
    "        d[c] = 0.0\n",
    "\n",
    "# If 14y bucket absent, approximate as 10y+20y (as in prior logic)\n",
    "if d[\"issu_14_bil\"].fillna(0.0).abs().sum() == 0.0:\n",
    "    d[\"issu_14_bil\"] = d.get(\"issu_10_bil\", 0.0) + d.get(\"issu_20_bil\", 0.0)\n",
    "\n",
    "for c in [\"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"]:\n",
    "    d[c] = pd.to_numeric(d[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# Keep only the issuance controls used in the design\n",
    "d = d[[\"date\", \"issu_7_bil\", \"issu_14_bil\", \"issu_30_bil\"]]\n",
    "fred = fred.groupby(\"date\", as_index=False).mean(numeric_only=True)\n",
    "repo = repo.groupby(\"date\", as_index=False).mean(numeric_only=True)\n",
    "d    = d.groupby(\"date\", as_index=False).sum(numeric_only=True)   # issuance is additive\n",
    "\n",
    "for col in [\"VIX\",\"HY_OAS\",\"BAA10Y\",\"SOFR\",\"spr_tgcr\",\"spr_effr\",\"tgcr\",\"effr\"]:\n",
    "    if col in fred.columns: fred[col] = coerce_num(fred[col])\n",
    "    if col in repo.columns: repo[col] = coerce_num(repo[col])\n",
    "\n",
    "\n",
    "# If 'controls' was not set from intermediate analysis_panel, build it from raw sources.\n",
    "if \"controls\" not in globals():\n",
    "    controls = fred.merge(repo, on=\"date\", how=\"outer\").merge(d, on=\"date\", how=\"outer\").sort_values(\"date\")\n",
    "    # keep only needed controls (drop extras like tgcr/effr if not needed)\n",
    "    keep = [\"date\"] + sorted(set(CONFIG[\"direct_controls\"]) & set(controls.columns))\n",
    "    controls = controls[keep].copy()\n",
    "    logger.info(\"Built controls from raw sources. columns=%s\", keep)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764af4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_long = arb_long.merge(controls, on=\"date\", how=\"left\")\n",
    "for c in CONFIG[\"direct_controls\"]:\n",
    "    if c in panel_long.columns:\n",
    "        panel_long[c] = pd.to_numeric(panel_long[c], errors=\"coerce\")\n",
    "panel_long.to_csv(run_dir / \"data\" / \"arb_panel_long.csv\", index=False)\n",
    "panel_long = panel_long[(panel_long[\"date\"] >= \"2019-01-01\") & (panel_long[\"date\"] <= \"2021-12-31\")]\n",
    "panel_long.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1A: summary stats by series and regime\n",
    "regimes = {\n",
    "    \"pre\": (pd.Timestamp(\"2019-01-01\"), pd.Timestamp(\"2020-03-31\")),\n",
    "    \"relief\": (pd.Timestamp(\"2020-04-01\"), pd.Timestamp(\"2021-03-31\")),\n",
    "    \"post\": (pd.Timestamp(\"2021-04-01\"), pd.Timestamp.max),\n",
    "}\n",
    "rows = []\n",
    "for series, g in panel_long.groupby(\"series\"):\n",
    "    g = g.sort_values(\"date\")\n",
    "    for regime, (start, end) in regimes.items():\n",
    "        sub = g[(g[\"date\"] >= start) & (g[\"date\"] <= end)][[\"date\", \"y\"]].dropna()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        lb_p = np.nan\n",
    "        try:\n",
    "            lb = acorr_ljungbox(sub[\"y\"], lags=[min(10, max(1, len(sub) // 5))], return_df=True)\n",
    "            lb_p = float(lb[\"lb_pvalue\"].iloc[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "        rows.append({\n",
    "            \"series\": str(series), \"regime\": regime,\n",
    "            \"sample_start\": sub[\"date\"].min(), \"sample_end\": sub[\"date\"].max(), \"N\": int(sub.shape[0]),\n",
    "            \"mean\": float(sub[\"y\"].mean()), \"std\": float(sub[\"y\"].std()),\n",
    "            \"p1\": float(sub[\"y\"].quantile(0.01)), \"p5\": float(sub[\"y\"].quantile(0.05)),\n",
    "            \"p50\": float(sub[\"y\"].quantile(0.50)), \"p95\": float(sub[\"y\"].quantile(0.95)), \"p99\": float(sub[\"y\"].quantile(0.99)),\n",
    "            \"autocorr1\": float(sub[\"y\"].autocorr(1)), \"ljungbox_pvalue\": lb_p,\n",
    "        })\n",
    "summary_stats = pd.DataFrame(rows)\n",
    "summary_stats.to_csv(run_dir / \"tables\" / \"summary_stats.csv\", index=False)\n",
    "summary_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1B: jump regressions (TOTAL vs DIRECT) by series\n",
    "jump_rows = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for window in CONFIG[\"windows\"]:\n",
    "        for series, g in panel_long.groupby(\"series\"):\n",
    "            for spec, controls_set in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "                est, se, n = jump_estimator(g, y_col=\"y\", event_date=event, window=window, controls=controls_set, hac_lags=CONFIG[\"hac_lags\"])\n",
    "                jump_rows.append({\n",
    "                    \"event\": event, \"window\": window, \"series\": str(series), \"spec\": spec,\n",
    "                    \"estimate\": est, \"se\": se,\n",
    "                    \"ci_low\": est - 1.96 * se if pd.notna(est) and pd.notna(se) else np.nan,\n",
    "                    \"ci_high\": est + 1.96 * se if pd.notna(est) and pd.notna(se) else np.nan,\n",
    "                    \"N\": n,\n",
    "                })\n",
    "jump_results = pd.DataFrame(jump_rows)\n",
    "jump_results.to_csv(run_dir / \"tables\" / \"jump_results.csv\", index=False)\n",
    "jump_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1C: binned event-study + plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _bin_mid(term: str):\n",
    "    m = re.search(r\"\\[\\s*(-?\\d+)\\s*,\\s*(-?\\d+)\\s*\\]\", str(term))\n",
    "    if not m:\n",
    "        return np.nan\n",
    "    a,b=int(m.group(1)), int(m.group(2))\n",
    "    return 0.5*(a+b)\n",
    "\n",
    "bin_rows = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    for series, g in panel_long.groupby(\"series\"):\n",
    "        for spec, controls_set in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "            es = event_study_regression(g, y_col=\"y\", event_date=event, bins=CONFIG[\"event_bins\"], controls=controls_set, hac_lags=CONFIG[\"hac_lags\"])\n",
    "            if es.empty:\n",
    "                continue\n",
    "            es = es.copy()\n",
    "            es[\"event\"] = event\n",
    "            es[\"series\"] = str(series)\n",
    "            es[\"spec\"] = spec\n",
    "            es[\"bin_mid\"] = es[\"term\"].apply(_bin_mid)\n",
    "            # enforce numeric for plotting\n",
    "            for c in [\"estimate\",\"ci_low\",\"ci_high\",\"bin_mid\"]:\n",
    "                if c in es.columns:\n",
    "                    es[c] = pd.to_numeric(es[c], errors=\"coerce\")\n",
    "            bin_rows.append(es)\n",
    "\n",
    "            plot_df = es.sort_values(\"bin_mid\").dropna(subset=[\"bin_mid\",\"estimate\",\"ci_low\",\"ci_high\"])\n",
    "            if plot_df.empty:\n",
    "                continue\n",
    "            fig, ax = plt.subplots(figsize=(8, 4))\n",
    "            ax.plot(plot_df[\"bin_mid\"].to_numpy(float), plot_df[\"estimate\"].to_numpy(float), marker=\"o\")\n",
    "            ax.fill_between(plot_df[\"bin_mid\"].to_numpy(float), plot_df[\"ci_low\"].to_numpy(float), plot_df[\"ci_high\"].to_numpy(float), alpha=0.2)\n",
    "            ax.axhline(0, color=\"black\", linewidth=1)\n",
    "            ax.axvline(0, color=\"black\", linewidth=1, linestyle=\"--\")\n",
    "            ax.set_xlabel(\"Event time (bin midpoint)\")\n",
    "            ax.set_title(f\"{CONFIG['series_label']} | Event={event} series={series} spec={spec}\")\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(run_dir / \"figures\" / f\"event_path_{slugify(CONFIG['series_label'])}_{slugify(series)}_{event}_{spec.lower()}.png\", dpi=150)\n",
    "            plt.close(fig)\n",
    "\n",
    "eventstudy_bins = pd.concat(bin_rows, ignore_index=True) if bin_rows else pd.DataFrame()\n",
    "eventstudy_bins.to_csv(run_dir / \"tables\" / \"eventstudy_bins.csv\", index=False)\n",
    "eventstudy_bins.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1D pooled regression with series FE + stargazer export\n",
    "pooled_rows = []\n",
    "models = []\n",
    "for event in CONFIG[\"events\"]:\n",
    "    sub = add_event_time(panel_long, event)\n",
    "    sub = sub[sub[\"event_time\"].between(-60, 60)].copy()\n",
    "    sub[\"post\"] = (sub[\"event_time\"] >= 0).astype(int)\n",
    "    for spec, controls_set in [(\"TOTAL\", CONFIG[\"total_controls\"]), (\"DIRECT\", CONFIG[\"direct_controls\"])]:\n",
    "        use_controls = [c for c in controls_set if c in sub.columns]\n",
    "        use_cols = [\"y\", \"post\", \"series\", *use_controls]\n",
    "        reg = sub[use_cols].dropna().copy()\n",
    "        if reg.empty:\n",
    "            continue\n",
    "        rhs = \"post + C(series)\"\n",
    "        if use_controls:\n",
    "            rhs += \" + \" + \" + \".join(use_controls)\n",
    "\n",
    "        # statsmodels robust fit (keeps robust bse attached)\n",
    "        res = ols(f\"y ~ {rhs}\", data=reg).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": CONFIG[\"hac_lags\"]})\n",
    "        models.append(res)\n",
    "\n",
    "        if \"post\" in res.params.index:\n",
    "            pooled_rows.append({\"event\": event, \"spec\": spec, \"post\": float(res.params[\"post\"]), \"se\": float(res.bse[\"post\"]), \"N\": int(res.nobs)})\n",
    "\n",
    "pd.DataFrame(pooled_rows).to_csv(run_dir / \"tables\" / \"pooled_jump_results.csv\", index=False)\n",
    "\n",
    "html_path = run_dir / \"tables\" / \"regression_table.html\"\n",
    "try:\n",
    "    import pandas as pd  # ensure available\n",
    "    import stargazer.stargazer as _st\n",
    "    _st.pd = pd  # monkey-patch: stargazer sometimes references pd without importing\n",
    "    from stargazer.stargazer import Stargazer\n",
    "    if models:\n",
    "        sg = Stargazer(models)\n",
    "        sg.title(f\"Pooled jump regressions (HAC SE) | {CONFIG['series_label']}\")\n",
    "        html_path.write_text(sg.render_html(), encoding=\"utf-8\")\n",
    "    else:\n",
    "        html_path.write_text(\"<html><body><p>No pooled models available.</p></body></html>\", encoding=\"utf-8\")\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    html_path.write_text(f\"<html><body><p>Stargazer unavailable: {exc}</p></body></html>\", encoding=\"utf-8\")\n",
    "\n",
    "pd.DataFrame(pooled_rows).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2 mechanism (weekly), skip gracefully if required data missing\n",
    "layer2_note = \"\"\n",
    "try:\n",
    "    pd_long = load_any_table(resolve_dataset_path(\"primary_dealer_stats_ofr_stfm_nypd_long\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "    bank = load_any_table(resolve_dataset_path(\"bank_exposure_y9c_agg_daily\", expected_dir=repo_root / \"data\" / \"raw\" / \"event_inputs\"))\n",
    "\n",
    "    pd_long[\"date\"] = pd.to_datetime(pd_long[\"date\"], errors=\"coerce\")\n",
    "    bank[\"date\"] = pd.to_datetime(bank[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    util_w = pd_long.pivot_table(index=\"date\", columns=\"mnemonic\", values=\"value\", aggfunc=\"mean\").resample(\"W-FRI\").mean()\n",
    "    util_w[\"utilization_index\"] = util_w.sum(axis=1, min_count=1)\n",
    "    util_w[\"utilization_lag1w\"] = util_w[\"utilization_index\"].shift(1)\n",
    "\n",
    "    bank_w = bank.set_index(\"date\").resample(\"W-FRI\").mean()[[\"agg_exempt_share\"]]\n",
    "\n",
    "    # Weekly mean spread across series (keeps design consistent with template)\n",
    "    y_w = panel_long.groupby([pd.Grouper(key=\"date\", freq=\"W-FRI\")])[\"y\"].mean().to_frame(\"y\")\n",
    "\n",
    "    c_w = panel_long.set_index(\"date\")[[c for c in CONFIG[\"direct_controls\"] if c in panel_long.columns]].resample(\"W-FRI\").mean()\n",
    "\n",
    "    mech = y_w.join([bank_w, util_w[[\"utilization_lag1w\"]], c_w], how=\"inner\").dropna()\n",
    "    mech[\"relief\"] = ((mech.index >= \"2020-04-01\") & (mech.index <= \"2021-03-31\")).astype(int)\n",
    "    mech[\"z_exempt\"] = (mech[\"agg_exempt_share\"] - mech[\"agg_exempt_share\"].mean()) / mech[\"agg_exempt_share\"].std()\n",
    "    mech[\"z_util_l1\"] = (mech[\"utilization_lag1w\"] - mech[\"utilization_lag1w\"].mean()) / mech[\"utilization_lag1w\"].std()\n",
    "    mech[\"relief_x_exempt\"] = mech[\"relief\"] * mech[\"z_exempt\"]\n",
    "    mech[\"relief_x_util\"] = mech[\"relief\"] * mech[\"z_util_l1\"]\n",
    "\n",
    "    rhs = \"relief + relief_x_exempt + relief_x_util\"\n",
    "    use_controls = [c for c in CONFIG[\"total_controls\"] + [c for c in CONFIG[\"direct_controls\"] if c not in CONFIG[\"total_controls\"]] if c in mech.columns]\n",
    "    if use_controls:\n",
    "        rhs += \" + \" + \" + \".join(use_controls)\n",
    "\n",
    "    res = ols(f\"y ~ {rhs}\", data=mech).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 4})\n",
    "    out = pd.DataFrame({\"term\": res.params.index, \"coef\": res.params.values, \"se\": res.bse.values})\n",
    "    out.to_csv(run_dir / \"tables\" / \"layer2_mechanism_weekly.csv\", index=False)\n",
    "    out.head()\n",
    "\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    layer2_note = f\"Layer 2 skipped: {exc}\"\n",
    "    (run_dir / \"tables\" / \"layer2_mechanism_weekly.csv\").write_text(layer2_note, encoding=\"utf-8\")\n",
    "\n",
    "layer2_note\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}